{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:01:13.569469Z",
     "start_time": "2024-03-12T20:01:10.867542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 different classes: Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop or Rock.\n",
      "objective 1: construct a classifier which, based on the features of a song, predicts its genre\n",
      "objective 2: estimate its generalisation error under the 0â€“1 loss.\n",
      "Features are real-valued, correspond to summary statistics (mean, sd, skewness, kurtosis, median, min, max) of \n",
      "time series of various music features, such as the chromagram or the Mel-frequency cepstrum.\n",
      "Feature description: \n",
      "\n",
      "Feature description: \n",
      "chroma_cens: Chroma Energy Normalized (CENS, 12 chroma) - 84 features\n",
      "chroma_cqt: Constant-Q chromagram (12 chroma) - 84 features\n",
      "chroma_stft: Chromagram (12 chroma) - 84 features\n",
      "mfcc: Mel-frequency cepstrum (20 coefficients) - 140 features\n",
      "rmse: Root-mean-square - 7 features\n",
      "spectral_bandwidth: Spectral bandwidth - 7 features\n",
      "spectral_centroid: Spectral centroid - 7 features\n",
      "spectral_contrast: Spectral contrast (7 frequency bands) - 49 features\n",
      "spectral_rolloff: Roll-off frequency - 7 features\n",
      "tonnetz: Tonal centroid features (6 features) - 42 features\n",
      "zcr: Zero-crossing rate - 7 features\n",
      "x_train: 6000 rows on 518 columns\n",
      "Objects loaded: x_train, x_test, y_train as pd dataframes, x_train_np, x_test_np, y_train_np as NP arrays\n",
      "function generate_submission_csv(genre_predictions, filename='submission.csv') available\n"
     ]
    }
   ],
   "source": [
    "%run 'Setup.py'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_np.ravel()) #\n",
    "\n",
    "# Split training data into training and temporary validation sets\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(x_train, y_train_encoded, test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the temporary validation set into validation and test sets\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)  \n",
    "X_real_test_scaled = scaler.transform(x_test_np)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:19:38.996581Z",
     "start_time": "2024-03-12T20:19:38.953462Z"
    }
   },
   "id": "f94100bb0cca3699",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KNN Bagging"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77bab842d2f3b693"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5222222222222223\n",
      "Validation accuracy: 0.4275\n",
      "Test accuracy: 0.39916666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Number of base estimators\n",
    "n_estimators = 10  # You can experiment with this number\n",
    "\n",
    "# Create a BaggingClassifier with KNN as base estimators\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=KNeighborsClassifier(n_neighbors=1), \n",
    "    n_estimators=n_estimators, \n",
    "    max_samples=1.0 / n_estimators, \n",
    "    bootstrap=True, \n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all cores\n",
    ")\n",
    "\n",
    "# Fit the BaggingClassifier\n",
    "bagging_clf.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = bagging_clf.score(X_train_scaled, Y_train)\n",
    "val_accuracy = bagging_clf.score(X_val_scaled, Y_val)\n",
    "test_accuracy = bagging_clf.score(X_test_scaled, Y_test)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict on real test set if needed\n",
    "real_test_predictions = bagging_clf.predict(X_real_test_scaled)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:06:58.695242Z",
     "start_time": "2024-03-12T20:06:57.694227Z"
    }
   },
   "id": "bbb22030f57699ee",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KNN  Ensembling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bece8d795713a559"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy with combined KNN models: 0.12666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Define a range of k values\n",
    "k_values = [1, 8, 64]\n",
    "\n",
    "# Train a KNN model for each k\n",
    "knn_models = [KNeighborsClassifier(n_neighbors=k).fit(X_train_scaled, Y_train) for k in k_values]\n",
    "\n",
    "# Predict on the validation set with each model\n",
    "val_predictions = [model.predict(X_val_scaled) for model in knn_models]\n",
    "\n",
    "# Combine predictions: Take the mode of predictions across the different models\n",
    "combined_val_predictions = mode(val_predictions, axis=0)[0][0]\n",
    "\n",
    "# Evaluate accuracy\n",
    "val_accuracy = np.mean(combined_val_predictions == Y_val)\n",
    "print(f\"Validation accuracy with combined KNN models: {val_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:17:01.746104Z",
     "start_time": "2024-03-12T20:17:01.635515Z"
    }
   },
   "id": "ffd2772d60f84629",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN on feature subsets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea15021f27297ff3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chroma_cens_kurtosis_01', 'chroma_cens_kurtosis_02', 'chroma_cens_kurtosis_03', 'chroma_cens_kurtosis_04', 'chroma_cens_kurtosis_05', 'chroma_cens_kurtosis_06', 'chroma_cens_kurtosis_07', 'chroma_cens_kurtosis_08', 'chroma_cens_kurtosis_09', 'chroma_cens_kurtosis_10', 'chroma_cens_kurtosis_11', 'chroma_cens_kurtosis_12', 'chroma_cens_max_01', 'chroma_cens_max_02', 'chroma_cens_max_03', 'chroma_cens_max_04', 'chroma_cens_max_05', 'chroma_cens_max_06', 'chroma_cens_max_07', 'chroma_cens_max_08', 'chroma_cens_max_09', 'chroma_cens_max_10', 'chroma_cens_max_11', 'chroma_cens_max_12', 'chroma_cens_mean_01', 'chroma_cens_mean_02', 'chroma_cens_mean_03', 'chroma_cens_mean_04', 'chroma_cens_mean_05', 'chroma_cens_mean_06', 'chroma_cens_mean_07', 'chroma_cens_mean_08', 'chroma_cens_mean_09', 'chroma_cens_mean_10', 'chroma_cens_mean_11', 'chroma_cens_mean_12', 'chroma_cens_median_01', 'chroma_cens_median_02', 'chroma_cens_median_03', 'chroma_cens_median_04', 'chroma_cens_median_05', 'chroma_cens_median_06', 'chroma_cens_median_07', 'chroma_cens_median_08', 'chroma_cens_median_09', 'chroma_cens_median_10', 'chroma_cens_median_11', 'chroma_cens_median_12', 'chroma_cens_min_01', 'chroma_cens_min_02', 'chroma_cens_min_03', 'chroma_cens_min_04', 'chroma_cens_min_05', 'chroma_cens_min_06', 'chroma_cens_min_07', 'chroma_cens_min_08', 'chroma_cens_min_09', 'chroma_cens_min_10', 'chroma_cens_min_11', 'chroma_cens_min_12', 'chroma_cens_skew_01', 'chroma_cens_skew_02', 'chroma_cens_skew_03', 'chroma_cens_skew_04', 'chroma_cens_skew_05', 'chroma_cens_skew_06', 'chroma_cens_skew_07', 'chroma_cens_skew_08', 'chroma_cens_skew_09', 'chroma_cens_skew_10', 'chroma_cens_skew_11', 'chroma_cens_skew_12', 'chroma_cens_std_01', 'chroma_cens_std_02', 'chroma_cens_std_03', 'chroma_cens_std_04', 'chroma_cens_std_05', 'chroma_cens_std_06', 'chroma_cens_std_07', 'chroma_cens_std_08', 'chroma_cens_std_09', 'chroma_cens_std_10', 'chroma_cens_std_11', 'chroma_cens_std_12', 'chroma_cqt_kurtosis_01', 'chroma_cqt_kurtosis_02', 'chroma_cqt_kurtosis_03', 'chroma_cqt_kurtosis_04', 'chroma_cqt_kurtosis_05', 'chroma_cqt_kurtosis_06', 'chroma_cqt_kurtosis_07', 'chroma_cqt_kurtosis_08', 'chroma_cqt_kurtosis_09', 'chroma_cqt_kurtosis_10', 'chroma_cqt_kurtosis_11', 'chroma_cqt_kurtosis_12', 'chroma_cqt_max_01', 'chroma_cqt_max_02', 'chroma_cqt_max_03', 'chroma_cqt_max_04', 'chroma_cqt_max_05', 'chroma_cqt_max_06', 'chroma_cqt_max_07', 'chroma_cqt_max_08', 'chroma_cqt_max_09', 'chroma_cqt_max_10', 'chroma_cqt_max_11', 'chroma_cqt_max_12', 'chroma_cqt_mean_01', 'chroma_cqt_mean_02', 'chroma_cqt_mean_03', 'chroma_cqt_mean_04', 'chroma_cqt_mean_05', 'chroma_cqt_mean_06', 'chroma_cqt_mean_07', 'chroma_cqt_mean_08', 'chroma_cqt_mean_09', 'chroma_cqt_mean_10', 'chroma_cqt_mean_11', 'chroma_cqt_mean_12', 'chroma_cqt_median_01', 'chroma_cqt_median_02', 'chroma_cqt_median_03', 'chroma_cqt_median_04', 'chroma_cqt_median_05', 'chroma_cqt_median_06', 'chroma_cqt_median_07', 'chroma_cqt_median_08', 'chroma_cqt_median_09', 'chroma_cqt_median_10', 'chroma_cqt_median_11', 'chroma_cqt_median_12', 'chroma_cqt_min_01', 'chroma_cqt_min_02', 'chroma_cqt_min_03', 'chroma_cqt_min_04', 'chroma_cqt_min_05', 'chroma_cqt_min_06', 'chroma_cqt_min_07', 'chroma_cqt_min_08', 'chroma_cqt_min_09', 'chroma_cqt_min_10', 'chroma_cqt_min_11', 'chroma_cqt_min_12', 'chroma_cqt_skew_01', 'chroma_cqt_skew_02', 'chroma_cqt_skew_03', 'chroma_cqt_skew_04', 'chroma_cqt_skew_05', 'chroma_cqt_skew_06', 'chroma_cqt_skew_07', 'chroma_cqt_skew_08', 'chroma_cqt_skew_09', 'chroma_cqt_skew_10', 'chroma_cqt_skew_11', 'chroma_cqt_skew_12', 'chroma_cqt_std_01', 'chroma_cqt_std_02', 'chroma_cqt_std_03', 'chroma_cqt_std_04', 'chroma_cqt_std_05', 'chroma_cqt_std_06', 'chroma_cqt_std_07', 'chroma_cqt_std_08', 'chroma_cqt_std_09', 'chroma_cqt_std_10', 'chroma_cqt_std_11', 'chroma_cqt_std_12', 'chroma_stft_kurtosis_01', 'chroma_stft_kurtosis_02', 'chroma_stft_kurtosis_03', 'chroma_stft_kurtosis_04', 'chroma_stft_kurtosis_05', 'chroma_stft_kurtosis_06', 'chroma_stft_kurtosis_07', 'chroma_stft_kurtosis_08', 'chroma_stft_kurtosis_09', 'chroma_stft_kurtosis_10', 'chroma_stft_kurtosis_11', 'chroma_stft_kurtosis_12', 'chroma_stft_max_01', 'chroma_stft_max_02', 'chroma_stft_max_03', 'chroma_stft_max_04', 'chroma_stft_max_05', 'chroma_stft_max_06', 'chroma_stft_max_07', 'chroma_stft_max_08', 'chroma_stft_max_09', 'chroma_stft_max_10', 'chroma_stft_max_11', 'chroma_stft_max_12', 'chroma_stft_mean_01', 'chroma_stft_mean_02', 'chroma_stft_mean_03', 'chroma_stft_mean_04', 'chroma_stft_mean_05', 'chroma_stft_mean_06', 'chroma_stft_mean_07', 'chroma_stft_mean_08', 'chroma_stft_mean_09', 'chroma_stft_mean_10', 'chroma_stft_mean_11', 'chroma_stft_mean_12', 'chroma_stft_median_01', 'chroma_stft_median_02', 'chroma_stft_median_03', 'chroma_stft_median_04', 'chroma_stft_median_05', 'chroma_stft_median_06', 'chroma_stft_median_07', 'chroma_stft_median_08', 'chroma_stft_median_09', 'chroma_stft_median_10', 'chroma_stft_median_11', 'chroma_stft_median_12', 'chroma_stft_min_01', 'chroma_stft_min_02', 'chroma_stft_min_03', 'chroma_stft_min_04', 'chroma_stft_min_05', 'chroma_stft_min_06', 'chroma_stft_min_07', 'chroma_stft_min_08', 'chroma_stft_min_09', 'chroma_stft_min_10', 'chroma_stft_min_11', 'chroma_stft_min_12', 'chroma_stft_skew_01', 'chroma_stft_skew_02', 'chroma_stft_skew_03', 'chroma_stft_skew_04', 'chroma_stft_skew_05', 'chroma_stft_skew_06', 'chroma_stft_skew_07', 'chroma_stft_skew_08', 'chroma_stft_skew_09', 'chroma_stft_skew_10', 'chroma_stft_skew_11', 'chroma_stft_skew_12', 'chroma_stft_std_01', 'chroma_stft_std_02', 'chroma_stft_std_03', 'chroma_stft_std_04', 'chroma_stft_std_05', 'chroma_stft_std_06', 'chroma_stft_std_07', 'chroma_stft_std_08', 'chroma_stft_std_09', 'chroma_stft_std_10', 'chroma_stft_std_11', 'chroma_stft_std_12', 'mfcc_kurtosis_01', 'mfcc_kurtosis_02', 'mfcc_kurtosis_03', 'mfcc_kurtosis_04', 'mfcc_kurtosis_05', 'mfcc_kurtosis_06', 'mfcc_kurtosis_07', 'mfcc_kurtosis_08', 'mfcc_kurtosis_09', 'mfcc_kurtosis_10', 'mfcc_kurtosis_11', 'mfcc_kurtosis_12', 'mfcc_kurtosis_13', 'mfcc_kurtosis_14', 'mfcc_kurtosis_15', 'mfcc_kurtosis_16', 'mfcc_kurtosis_17', 'mfcc_kurtosis_18', 'mfcc_kurtosis_19', 'mfcc_kurtosis_20', 'mfcc_max_01', 'mfcc_max_02', 'mfcc_max_03', 'mfcc_max_04', 'mfcc_max_05', 'mfcc_max_06', 'mfcc_max_07', 'mfcc_max_08', 'mfcc_max_09', 'mfcc_max_10', 'mfcc_max_11', 'mfcc_max_12', 'mfcc_max_13', 'mfcc_max_14', 'mfcc_max_15', 'mfcc_max_16', 'mfcc_max_17', 'mfcc_max_18', 'mfcc_max_19', 'mfcc_max_20', 'mfcc_mean_01', 'mfcc_mean_02', 'mfcc_mean_03', 'mfcc_mean_04', 'mfcc_mean_05', 'mfcc_mean_06', 'mfcc_mean_07', 'mfcc_mean_08', 'mfcc_mean_09', 'mfcc_mean_10', 'mfcc_mean_11', 'mfcc_mean_12', 'mfcc_mean_13', 'mfcc_mean_14', 'mfcc_mean_15', 'mfcc_mean_16', 'mfcc_mean_17', 'mfcc_mean_18', 'mfcc_mean_19', 'mfcc_mean_20', 'mfcc_median_01', 'mfcc_median_02', 'mfcc_median_03', 'mfcc_median_04', 'mfcc_median_05', 'mfcc_median_06', 'mfcc_median_07', 'mfcc_median_08', 'mfcc_median_09', 'mfcc_median_10', 'mfcc_median_11', 'mfcc_median_12', 'mfcc_median_13', 'mfcc_median_14', 'mfcc_median_15', 'mfcc_median_16', 'mfcc_median_17', 'mfcc_median_18', 'mfcc_median_19', 'mfcc_median_20', 'mfcc_min_01', 'mfcc_min_02', 'mfcc_min_03', 'mfcc_min_04', 'mfcc_min_05', 'mfcc_min_06', 'mfcc_min_07', 'mfcc_min_08', 'mfcc_min_09', 'mfcc_min_10', 'mfcc_min_11', 'mfcc_min_12', 'mfcc_min_13', 'mfcc_min_14', 'mfcc_min_15', 'mfcc_min_16', 'mfcc_min_17', 'mfcc_min_18', 'mfcc_min_19', 'mfcc_min_20', 'mfcc_skew_01', 'mfcc_skew_02', 'mfcc_skew_03', 'mfcc_skew_04', 'mfcc_skew_05', 'mfcc_skew_06', 'mfcc_skew_07', 'mfcc_skew_08', 'mfcc_skew_09', 'mfcc_skew_10', 'mfcc_skew_11', 'mfcc_skew_12', 'mfcc_skew_13', 'mfcc_skew_14', 'mfcc_skew_15', 'mfcc_skew_16', 'mfcc_skew_17', 'mfcc_skew_18', 'mfcc_skew_19', 'mfcc_skew_20', 'mfcc_std_01', 'mfcc_std_02', 'mfcc_std_03', 'mfcc_std_04', 'mfcc_std_05', 'mfcc_std_06', 'mfcc_std_07', 'mfcc_std_08', 'mfcc_std_09', 'mfcc_std_10', 'mfcc_std_11', 'mfcc_std_12', 'mfcc_std_13', 'mfcc_std_14', 'mfcc_std_15', 'mfcc_std_16', 'mfcc_std_17', 'mfcc_std_18', 'mfcc_std_19', 'mfcc_std_20', 'rmse_kurtosis_01', 'rmse_max_01', 'rmse_mean_01', 'rmse_median_01', 'rmse_min_01', 'rmse_skew_01', 'rmse_std_01', 'spectral_bandwidth_kurtosis_01', 'spectral_bandwidth_max_01', 'spectral_bandwidth_mean_01', 'spectral_bandwidth_median_01', 'spectral_bandwidth_min_01', 'spectral_bandwidth_skew_01', 'spectral_bandwidth_std_01', 'spectral_centroid_kurtosis_01', 'spectral_centroid_max_01', 'spectral_centroid_mean_01', 'spectral_centroid_median_01', 'spectral_centroid_min_01', 'spectral_centroid_skew_01', 'spectral_centroid_std_01', 'spectral_contrast_kurtosis_01', 'spectral_contrast_kurtosis_02', 'spectral_contrast_kurtosis_03', 'spectral_contrast_kurtosis_04', 'spectral_contrast_kurtosis_05', 'spectral_contrast_kurtosis_06', 'spectral_contrast_kurtosis_07', 'spectral_contrast_max_01', 'spectral_contrast_max_02', 'spectral_contrast_max_03', 'spectral_contrast_max_04', 'spectral_contrast_max_05', 'spectral_contrast_max_06', 'spectral_contrast_max_07', 'spectral_contrast_mean_01', 'spectral_contrast_mean_02', 'spectral_contrast_mean_03', 'spectral_contrast_mean_04', 'spectral_contrast_mean_05', 'spectral_contrast_mean_06', 'spectral_contrast_mean_07', 'spectral_contrast_median_01', 'spectral_contrast_median_02', 'spectral_contrast_median_03', 'spectral_contrast_median_04', 'spectral_contrast_median_05', 'spectral_contrast_median_06', 'spectral_contrast_median_07', 'spectral_contrast_min_01', 'spectral_contrast_min_02', 'spectral_contrast_min_03', 'spectral_contrast_min_04', 'spectral_contrast_min_05', 'spectral_contrast_min_06', 'spectral_contrast_min_07', 'spectral_contrast_skew_01', 'spectral_contrast_skew_02', 'spectral_contrast_skew_03', 'spectral_contrast_skew_04', 'spectral_contrast_skew_05', 'spectral_contrast_skew_06', 'spectral_contrast_skew_07', 'spectral_contrast_std_01', 'spectral_contrast_std_02', 'spectral_contrast_std_03', 'spectral_contrast_std_04', 'spectral_contrast_std_05', 'spectral_contrast_std_06', 'spectral_contrast_std_07', 'spectral_rolloff_kurtosis_01', 'spectral_rolloff_max_01', 'spectral_rolloff_mean_01', 'spectral_rolloff_median_01', 'spectral_rolloff_min_01', 'spectral_rolloff_skew_01', 'spectral_rolloff_std_01', 'tonnetz_kurtosis_01', 'tonnetz_kurtosis_02', 'tonnetz_kurtosis_03', 'tonnetz_kurtosis_04', 'tonnetz_kurtosis_05', 'tonnetz_kurtosis_06', 'tonnetz_max_01', 'tonnetz_max_02', 'tonnetz_max_03', 'tonnetz_max_04', 'tonnetz_max_05', 'tonnetz_max_06', 'tonnetz_mean_01', 'tonnetz_mean_02', 'tonnetz_mean_03', 'tonnetz_mean_04', 'tonnetz_mean_05', 'tonnetz_mean_06', 'tonnetz_median_01', 'tonnetz_median_02', 'tonnetz_median_03', 'tonnetz_median_04', 'tonnetz_median_05', 'tonnetz_median_06', 'tonnetz_min_01', 'tonnetz_min_02', 'tonnetz_min_03', 'tonnetz_min_04', 'tonnetz_min_05', 'tonnetz_min_06', 'tonnetz_skew_01', 'tonnetz_skew_02', 'tonnetz_skew_03', 'tonnetz_skew_04', 'tonnetz_skew_05', 'tonnetz_skew_06', 'tonnetz_std_01', 'tonnetz_std_02', 'tonnetz_std_03', 'tonnetz_std_04', 'tonnetz_std_05', 'tonnetz_std_06', 'zcr_kurtosis_01', 'zcr_max_01', 'zcr_mean_01', 'zcr_median_01', 'zcr_min_01', 'zcr_skew_01', 'zcr_std_01']\n"
     ]
    }
   ],
   "source": [
    "print(x_train.columns.tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:22:33.617029Z",
     "start_time": "2024-03-12T20:22:33.613919Z"
    }
   },
   "id": "fc3a25b625426abf",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function to extract feature subsets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23fd89e29b72564f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def split_features_by_type(X, feature_structure):\n",
    "    \"\"\"\n",
    "    Splits the dataset into subsets based on the feature structure provided.\n",
    "\n",
    "    :param X: numpy array, the dataset to be split (features only)\n",
    "    :param feature_structure: dict, keys are feature names and values are the number of features of that type\n",
    "    :return: dict of feature subsets\n",
    "    \"\"\"\n",
    "    feature_subsets = {}\n",
    "    start_idx = 0\n",
    "    \n",
    "    for feature_name, feature_count in feature_structure.items():\n",
    "        end_idx = start_idx + feature_count\n",
    "        feature_subsets[feature_name] = X[:, start_idx:end_idx]\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    return feature_subsets\n",
    "\n",
    "# Define the structure of your features based on the information you've provided\n",
    "feature_structure = {\n",
    "    'chroma_cens': 84,\n",
    "    'chroma_cqt': 84,\n",
    "    'chroma_stft': 84,\n",
    "    'mfcc': 140,\n",
    "    'rmse': 7,\n",
    "    'spectral_bandwidth': 7,\n",
    "    'spectral_centroid': 7,\n",
    "    'spectral_contrast': 49,\n",
    "    'spectral_rolloff': 7,\n",
    "    'tonnetz': 42,\n",
    "    'zcr': 7\n",
    "}\n",
    "\n",
    "# Example usage with a hypothetical dataset X_train_scaled\n",
    "# This would be your preprocessed and scaled training data as a NumPy array\n",
    "feature_subsets = split_features_by_type(X_train_scaled, feature_structure)\n",
    "\n",
    "# Now feature_subsets is a dictionary where, for example,\n",
    "# feature_subsets['mfcc'] contains only the MFCC features of the dataset.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:28:57.635853Z",
     "start_time": "2024-03-12T20:28:57.628390Z"
    }
   },
   "id": "8ef4b58614d4091c",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K for chroma_cens features: 9 with cross-validation score: 0.2663888888888889\n",
      "Best K for chroma_cqt features: 13 with cross-validation score: 0.27361111111111114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Dictionary to store the trained KNN models for each feature subset\n",
    "knn_models = {}\n",
    "\n",
    "# Train a KNN model for each feature subset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Dictionary to store the best KNN models for each feature subset\n",
    "best_knn_models = {}\n",
    "\n",
    "# Train a KNN model for each feature subset and find the best k using cross-validation\n",
    "for feature_name, X_subset in feature_subsets.items():\n",
    "    best_score = 0\n",
    "    best_k = 1\n",
    "    # Try different values of k\n",
    "    for k in range(1, 16):  # Let's try k from 1 to 15 as an example\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        scores = cross_val_score(knn, X_subset, Y_train, cv=5)\n",
    "        mean_score = scores.mean()\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "\n",
    "    # Train a new KNN model on the full training set with the best k\n",
    "    best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "    best_knn.fit(X_subset, Y_train)\n",
    "    best_knn_models[feature_name] = best_knn\n",
    "    print(f\"Best K for {feature_name} features: {best_k} with cross-validation score: {best_score}\")\n",
    "\n",
    "# Now best_knn_models dictionary contains the best KNN model for each feature subset\n",
    "\n",
    "\n",
    "# Now knn_models dictionary contains a trained KNN model for each feature subset\n",
    "# For example, knn_models['mfcc'] is the KNN model trained on the MFCC features\n",
    "\n",
    "# To make predictions, use the corresponding model for each feature subset\n",
    "# For instance, for MFCC features:\n",
    "# predictions_mfcc = knn_models['mfcc'].predict(feature_subsets['mfcc'])\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Assume we have a validation set X_val_scaled\n",
    "# Split it using the same function we defined earlier\n",
    "val_feature_subsets = split_features_by_type(X_val_scaled, feature_structure)\n",
    "\n",
    "# Gather predictions from all models on the validation set\n",
    "val_predictions = []\n",
    "for feature_name, model in best_knn_models.items():\n",
    "    # Ensure that we predict on the correct feature subset\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    predictions = model.predict(X_val_subset)\n",
    "    val_predictions.append(predictions)\n",
    "\n",
    "# Combine predictions using majority voting\n",
    "combined_val_predictions = mode(val_predictions, axis=0).mode\n",
    "\n",
    "# Calculate accuracy or any other metric based on the combined predictions\n",
    "val_accuracy = np.mean(combined_val_predictions.ravel() == Y_val)\n",
    "print(f\"Validation accuracy with combined KNN models: {val_accuracy}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:42:39.146100Z",
     "start_time": "2024-03-12T20:42:29.327209Z"
    }
   },
   "id": "252352816cfb937b",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weighted Voting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ca89f88800fb83c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.27\n",
      "Validation accuracy for chroma_cqt features: 0.2658333333333333\n",
      "Validation accuracy for chroma_stft features: 0.325\n",
      "Validation accuracy for mfcc features: 0.4741666666666667\n",
      "Validation accuracy for rmse features: 0.22916666666666666\n",
      "Validation accuracy for spectral_bandwidth features: 0.29583333333333334\n",
      "Validation accuracy for spectral_centroid features: 0.3416666666666667\n",
      "Validation accuracy for spectral_contrast features: 0.41083333333333333\n",
      "Validation accuracy for spectral_rolloff features: 0.3225\n",
      "Validation accuracy for tonnetz features: 0.25\n",
      "Validation accuracy for zcr features: 0.295\n",
      "Test accuracy with combined KNN models using weighted voting: 0.5208333333333334\n"
     ]
    }
   ],
   "source": [
    "# Calculate the validation accuracy for each feature subset and use it as weight for voting\n",
    "weights = []\n",
    "for feature_name, model in best_knn_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy}\")\n",
    "\n",
    "# Normalize weights so they sum up to 1\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# Split the test set using the same feature structure\n",
    "test_feature_subsets = split_features_by_type(X_test_scaled, feature_structure)\n",
    "\n",
    "# Predict on the test set with each model and weight the predictions\n",
    "weighted_test_predictions = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_knn_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * weights[i]\n",
    "    weighted_test_predictions += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions by taking the argmax to get final predictions\n",
    "combined_test_predictions = np.argmax(weighted_test_predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy based on the combined weighted predictions\n",
    "test_accuracy = np.mean(combined_test_predictions == Y_test)\n",
    "print(f\"Test accuracy with combined KNN models using weighted voting: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T20:53:52.557339Z",
     "start_time": "2024-03-12T20:53:52.074894Z"
    }
   },
   "id": "3a06d7017256015a",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination for chroma_cens features: k=9, metric=euclidean with cross-validation score: 0.27\n",
      "Best combination for chroma_cqt features: k=13, metric=euclidean with cross-validation score: 0.27\n",
      "Best combination for chroma_stft features: k=13, metric=euclidean with cross-validation score: 0.30\n",
      "Best combination for mfcc features: k=15, metric=manhattan with cross-validation score: 0.48\n",
      "Best combination for rmse features: k=14, metric=manhattan with cross-validation score: 0.24\n",
      "Best combination for spectral_bandwidth features: k=15, metric=manhattan with cross-validation score: 0.29\n",
      "Best combination for spectral_centroid features: k=14, metric=manhattan with cross-validation score: 0.33\n",
      "Best combination for spectral_contrast features: k=13, metric=manhattan with cross-validation score: 0.42\n",
      "Best combination for spectral_rolloff features: k=14, metric=manhattan with cross-validation score: 0.31\n",
      "Best combination for tonnetz features: k=14, metric=manhattan with cross-validation score: 0.29\n",
      "Best combination for zcr features: k=15, metric=manhattan with cross-validation score: 0.31\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define a list of distance metrics to try\n",
    "distance_metrics = ['euclidean', 'manhattan', 'minkowski']\n",
    "\n",
    "# Define a range of k values to try\n",
    "k_values = range(1, 16)  # Example: trying k from 1 to 15\n",
    "\n",
    "# Dictionary to store the best KNN model for each feature subset\n",
    "best_knn_models_dm_k = {}\n",
    "\n",
    "# Dictionary to store the best combination of distance metric and k for each feature subset\n",
    "best_combinations = {}\n",
    "\n",
    "for feature_name, X_subset in feature_subsets.items():\n",
    "    best_score = 0\n",
    "    best_combination = {'metric': '', 'k': 0}\n",
    "    # Iterate over each distance metric\n",
    "    for metric in distance_metrics:\n",
    "        # Iterate over each value of k\n",
    "        for k in k_values:\n",
    "            # Create a KNN model with the current metric and k\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "            # Perform cross-validation and compute the mean score\n",
    "            scores = cross_val_score(knn, X_subset, Y_train, cv=5)\n",
    "            mean_score = scores.mean()\n",
    "            # Update the best combination if the current model performs better\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_combination['metric'] = metric\n",
    "                best_combination['k'] = k\n",
    "    \n",
    "    # Once the best combination is found, retrain the model on the full training set\n",
    "    best_knn = KNeighborsClassifier(n_neighbors=best_combination['k'], metric=best_combination['metric'])\n",
    "    best_knn.fit(X_subset, Y_train)\n",
    "    best_knn_models_dm_k[feature_name] = best_knn\n",
    "    best_combinations[feature_name] = best_combination\n",
    "    print(f\"Best combination for {feature_name} features: k={best_combination['k']}, metric={best_combination['metric']} with cross-validation score: {best_score:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:06:27.271296Z",
     "start_time": "2024-03-12T21:05:42.932431Z"
    }
   },
   "id": "4e7dc5664d532fb8",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.27\n",
      "Validation accuracy for chroma_cqt features: 0.27\n",
      "Validation accuracy for chroma_stft features: 0.33\n",
      "Validation accuracy for mfcc features: 0.49\n",
      "Validation accuracy for rmse features: 0.23\n",
      "Validation accuracy for spectral_bandwidth features: 0.31\n",
      "Validation accuracy for spectral_centroid features: 0.35\n",
      "Validation accuracy for spectral_contrast features: 0.42\n",
      "Validation accuracy for spectral_rolloff features: 0.33\n",
      "Validation accuracy for tonnetz features: 0.28\n",
      "Validation accuracy for zcr features: 0.31\n",
      "Test accuracy with combined KNN models using weighted voting: 0.5116666666666667\n"
     ]
    }
   ],
   "source": [
    "# Calculate the validation accuracy for each feature subset and use it as weight for voting\n",
    "weights = []\n",
    "for feature_name, model in best_knn_models_dm_k.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy:.2f}\")\n",
    "\n",
    "# Normalize weights so they sum up to 1\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# Split the test set using the same feature structure\n",
    "test_feature_subsets = split_features_by_type(X_test_scaled, feature_structure)\n",
    "\n",
    "# Predict on the test set with each model and weight the predictions\n",
    "weighted_test_predictions = np.zeros((X_test_scaled.shape[0], 8), dtype=float)\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_knn_models_dm_k.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * weights[i]\n",
    "    weighted_test_predictions += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions by taking the argmax to get final predictions\n",
    "combined_test_predictions = np.argmax(weighted_test_predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy based on the combined weighted predictions\n",
    "test_accuracy = np.mean(combined_test_predictions == Y_test)\n",
    "print(f\"Test accuracy with combined KNN models using weighted voting: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:07:03.185478Z",
     "start_time": "2024-03-12T21:07:02.518301Z"
    }
   },
   "id": "9910e864df63134e",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination for chroma_cens features: k=9, metric=euclidean with cross-validation score: 0.27\n",
      "Best combination for chroma_cqt features: k=13, metric=euclidean with cross-validation score: 0.27\n",
      "Best combination for chroma_stft features: k=13, metric=euclidean with cross-validation score: 0.30\n",
      "Best combination for mfcc features: k=15, metric=manhattan with cross-validation score: 0.48\n",
      "Best combination for rmse features: k=14, metric=manhattan with cross-validation score: 0.24\n",
      "Best combination for spectral_bandwidth features: k=15, metric=manhattan with cross-validation score: 0.29\n",
      "Best combination for spectral_centroid features: k=14, metric=manhattan with cross-validation score: 0.33\n",
      "Best combination for spectral_contrast features: k=13, metric=manhattan with cross-validation score: 0.42\n",
      "Best combination for spectral_rolloff features: k=14, metric=manhattan with cross-validation score: 0.31\n",
      "Best combination for tonnetz features: k=14, metric=manhattan with cross-validation score: 0.29\n",
      "Best combination for zcr features: k=15, metric=manhattan with cross-validation score: 0.31\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define a list of distance metrics to try\n",
    "distance_metrics = ['euclidean', 'manhattan', 'minkowski']\n",
    "\n",
    "# Define a range of k values to try\n",
    "k_values = range(1, 16)  # Example: trying k from 1 to 15\n",
    "\n",
    "# Dictionary to store the best KNN model for each feature subset\n",
    "best_knn_models_dm_k = {}\n",
    "\n",
    "# Dictionary to store the best combination of distance metric and k for each feature subset\n",
    "best_combinations = {}\n",
    "cv_scores = []\n",
    "\n",
    "for feature_name, X_subset in feature_subsets.items():\n",
    "    best_score = 0\n",
    "    best_combination = {'metric': '', 'k': 0}\n",
    "    # Iterate over each distance metric\n",
    "    for metric in distance_metrics:\n",
    "        # Iterate over each value of k\n",
    "        for k in k_values:\n",
    "            # Create a KNN model with the current metric and k\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "            # Perform cross-validation and compute the mean score\n",
    "            scores = cross_val_score(knn, X_subset, Y_train, cv=5)\n",
    "            mean_score = scores.mean()\n",
    "            # Update the best combination if the current model performs better\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_combination['metric'] = metric\n",
    "                best_combination['k'] = k\n",
    "        \n",
    "    # Once the best combination is found, retrain the model on the full training set\n",
    "    best_knn = KNeighborsClassifier(n_neighbors=best_combination['k'], metric=best_combination['metric'])\n",
    "    best_knn.fit(X_subset, Y_train)\n",
    "    best_knn_models_dm_k[feature_name] = best_knn\n",
    "    best_combinations[feature_name] = best_combination\n",
    "    print(f\"Best combination for {feature_name} features: k={best_combination['k']}, metric={best_combination['metric']} with cross-validation score: {best_score:.2f}\")\n",
    "    cv_scores.append(best_score)\n",
    "    \n",
    "weights = cv_scores / np.sum(cv_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:18:05.664224Z",
     "start_time": "2024-03-12T21:17:20.548354Z"
    }
   },
   "id": "2e726d0b5198b3c7",
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use Logistic Reg as Meta-Learner "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fa34b7c64a31056"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked KNN models: 0.5533333333333333\n"
     ]
    }
   ],
   "source": [
    "# Generate meta-features for the validation set\n",
    "val_meta_features = np.column_stack([\n",
    "    model.predict_proba(val_feature_subsets[feature_name]) for feature_name, model in best_knn_models_dm_k.items()\n",
    "])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "meta_model.fit(val_meta_features, Y_val)  # Assuming Y_val is the validation set labels\n",
    "\n",
    "# Generate meta-features for the test set\n",
    "test_meta_features = np.column_stack([\n",
    "    model.predict_proba(test_feature_subsets[feature_name]) for feature_name, model in best_knn_models_dm_k.items()\n",
    "])\n",
    "\n",
    "# Final predictions using the meta-learner\n",
    "final_predictions = meta_model.predict(test_meta_features)\n",
    "\n",
    "# Calculate accuracy based on the combined weighted predictions\n",
    "test_accuracy = np.mean(final_predictions == Y_test)\n",
    "print(f\"Test accuracy with stacked KNN models: {test_accuracy}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:18:38.773860Z",
     "start_time": "2024-03-12T21:18:37.786030Z"
    }
   },
   "id": "38c5470b0825b228",
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use simple NN as meta-learner"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3dde12fb2e4eff8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Check if MPS (Apple Silicon GPU) is available, otherwise use CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:22:17.222560Z",
     "start_time": "2024-03-12T21:22:17.186426Z"
    }
   },
   "id": "562ef14a3ec414a2",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert meta-features and labels to PyTorch tensors\n",
    "X_val_tensor = torch.tensor(val_meta_features).float().to(device)\n",
    "Y_val_tensor = torch.tensor(Y_val).long().to(device)  # Assuming Y_val is already encoded as integers\n",
    "\n",
    "X_test_tensor = torch.tensor(test_meta_features).float().to(device)\n",
    "\n",
    "# Create DataLoader for the validation set\n",
    "batch_size = 32\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:22:22.450315Z",
     "start_time": "2024-03-12T21:22:22.397661Z"
    }
   },
   "id": "fb41281c65de18d2",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MetaLearnerNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MetaLearnerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = val_meta_features.shape[1]  # Number of meta-features\n",
    "num_classes = 8  # Assuming 8 output classes\n",
    "model = MetaLearnerNN(input_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:22:28.639645Z",
     "start_time": "2024-03-12T21:22:27.944415Z"
    }
   },
   "id": "efec9df7b67bf7f8",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.0166\n",
      "Epoch [2/100], Loss: 1.9187\n",
      "Epoch [3/100], Loss: 1.2562\n",
      "Epoch [4/100], Loss: 1.5095\n",
      "Epoch [5/100], Loss: 1.6083\n",
      "Epoch [6/100], Loss: 1.4226\n",
      "Epoch [7/100], Loss: 1.3978\n",
      "Epoch [8/100], Loss: 0.8296\n",
      "Epoch [9/100], Loss: 1.1557\n",
      "Epoch [10/100], Loss: 1.5398\n",
      "Epoch [11/100], Loss: 1.2438\n",
      "Epoch [12/100], Loss: 0.9344\n",
      "Epoch [13/100], Loss: 0.9194\n",
      "Epoch [14/100], Loss: 1.4493\n",
      "Epoch [15/100], Loss: 0.7873\n",
      "Epoch [16/100], Loss: 0.8417\n",
      "Epoch [17/100], Loss: 1.3168\n",
      "Epoch [18/100], Loss: 1.0663\n",
      "Epoch [19/100], Loss: 1.1252\n",
      "Epoch [20/100], Loss: 1.0421\n",
      "Epoch [21/100], Loss: 0.9747\n",
      "Epoch [22/100], Loss: 0.9928\n",
      "Epoch [23/100], Loss: 1.2502\n",
      "Epoch [24/100], Loss: 1.7411\n",
      "Epoch [25/100], Loss: 1.5389\n",
      "Epoch [26/100], Loss: 1.1095\n",
      "Epoch [27/100], Loss: 1.0328\n",
      "Epoch [28/100], Loss: 1.2746\n",
      "Epoch [29/100], Loss: 1.2590\n",
      "Epoch [30/100], Loss: 1.7449\n",
      "Epoch [31/100], Loss: 1.4318\n",
      "Epoch [32/100], Loss: 0.9114\n",
      "Epoch [33/100], Loss: 1.1084\n",
      "Epoch [34/100], Loss: 0.8470\n",
      "Epoch [35/100], Loss: 0.8258\n",
      "Epoch [36/100], Loss: 1.0526\n",
      "Epoch [37/100], Loss: 1.0925\n",
      "Epoch [38/100], Loss: 1.1202\n",
      "Epoch [39/100], Loss: 1.3509\n",
      "Epoch [40/100], Loss: 0.6464\n",
      "Epoch [41/100], Loss: 0.9668\n",
      "Epoch [42/100], Loss: 1.0219\n",
      "Epoch [43/100], Loss: 0.9777\n",
      "Epoch [44/100], Loss: 0.2867\n",
      "Epoch [45/100], Loss: 0.7567\n",
      "Epoch [46/100], Loss: 0.7230\n",
      "Epoch [47/100], Loss: 1.1568\n",
      "Epoch [48/100], Loss: 0.7856\n",
      "Epoch [49/100], Loss: 0.8238\n",
      "Epoch [50/100], Loss: 1.7736\n",
      "Epoch [51/100], Loss: 0.8788\n",
      "Epoch [52/100], Loss: 0.9907\n",
      "Epoch [53/100], Loss: 1.0178\n",
      "Epoch [54/100], Loss: 1.1698\n",
      "Epoch [55/100], Loss: 0.9748\n",
      "Epoch [56/100], Loss: 0.8994\n",
      "Epoch [57/100], Loss: 1.4373\n",
      "Epoch [58/100], Loss: 0.5771\n",
      "Epoch [59/100], Loss: 0.5173\n",
      "Epoch [60/100], Loss: 0.7687\n",
      "Epoch [61/100], Loss: 1.2547\n",
      "Epoch [62/100], Loss: 1.0092\n",
      "Epoch [63/100], Loss: 0.9452\n",
      "Epoch [64/100], Loss: 0.9828\n",
      "Epoch [65/100], Loss: 1.0249\n",
      "Epoch [66/100], Loss: 1.1308\n",
      "Epoch [67/100], Loss: 1.1428\n",
      "Epoch [68/100], Loss: 0.7573\n",
      "Epoch [69/100], Loss: 1.0698\n",
      "Epoch [70/100], Loss: 1.1911\n",
      "Epoch [71/100], Loss: 1.0676\n",
      "Epoch [72/100], Loss: 0.9901\n",
      "Epoch [73/100], Loss: 0.8216\n",
      "Epoch [74/100], Loss: 1.1957\n",
      "Epoch [75/100], Loss: 1.1687\n",
      "Epoch [76/100], Loss: 0.6853\n",
      "Epoch [77/100], Loss: 0.6341\n",
      "Epoch [78/100], Loss: 0.8354\n",
      "Epoch [79/100], Loss: 0.7992\n",
      "Epoch [80/100], Loss: 0.6653\n",
      "Epoch [81/100], Loss: 1.0082\n",
      "Epoch [82/100], Loss: 0.5524\n",
      "Epoch [83/100], Loss: 0.8321\n",
      "Epoch [84/100], Loss: 0.9214\n",
      "Epoch [85/100], Loss: 0.8755\n",
      "Epoch [86/100], Loss: 0.5639\n",
      "Epoch [87/100], Loss: 0.8344\n",
      "Epoch [88/100], Loss: 0.7348\n",
      "Epoch [89/100], Loss: 0.8493\n",
      "Epoch [90/100], Loss: 1.0263\n",
      "Epoch [91/100], Loss: 1.2003\n",
      "Epoch [92/100], Loss: 0.8065\n",
      "Epoch [93/100], Loss: 0.8159\n",
      "Epoch [94/100], Loss: 0.8378\n",
      "Epoch [95/100], Loss: 1.2353\n",
      "Epoch [96/100], Loss: 0.7854\n",
      "Epoch [97/100], Loss: 0.7163\n",
      "Epoch [98/100], Loss: 0.9060\n",
      "Epoch [99/100], Loss: 1.0857\n",
      "Epoch [100/100], Loss: 0.7190\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in val_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:23:03.396981Z",
     "start_time": "2024-03-12T21:22:42.412052Z"
    }
   },
   "id": "b3b78b2cd2bf30fc",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with PyTorch NN stacked model: 0.5266666666666666\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# Convert predictions back to numpy for accuracy calculation\n",
    "predicted_np = predicted.cpu().numpy()\n",
    "test_accuracy = (predicted_np == Y_test).mean()\n",
    "print(f\"Test accuracy with PyTorch NN stacked model: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T21:23:16.922384Z",
     "start_time": "2024-03-12T21:23:16.717426Z"
    }
   },
   "id": "8cc4d4b1938d8e9b",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b0d497a6d25e062d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
