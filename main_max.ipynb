{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da6dca2cae8faa76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, auc\n",
    "import scikitplot as skplt  # search for scikit-plot\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the training data and the test inputs\n",
    "x_train = pd.read_csv('X_train.csv', index_col=0, header=[0, 1, 2])\n",
    "x_train_np = np.array(x_train)\n",
    "y_train = pd.read_csv('y_train.csv', index_col=0)\n",
    "y_train_np = y_train.squeeze().to_numpy()  # Make y_train a NumPy array\n",
    "x_test = pd.read_csv('X_test.csv', index_col=0, header=[0, 1, 2])\n",
    "x_test_np = np.array(x_test)\n",
    "\n",
    "x_train_flat_columns = ['_'.join(col).strip() for col in x_train.columns.values]\n",
    "x_train.columns = x_train_flat_columns\n",
    "\n",
    "x_test_flat_columns = ['_'.join(col).strip() for col in x_test.columns.values]\n",
    "x_test.columns = x_train_flat_columns\n",
    "# Prepare data\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_np.ravel())\n",
    "\n",
    "# Split training data into training and temporary validation sets\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(x_train, y_train_encoded, test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the temporary validation set into validation and fake test set\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_real_test_scaled = scaler.transform(x_test)  # real test set we don't have labels for\n",
    "\n",
    "\n",
    "def calculate_test_accuracy(predictions):\n",
    "    return f\"Pseudo Test Set accuracy: {accuracy_score(Y_test, predictions):.2f}\"\n",
    "\n",
    "\n",
    "def calculate_trainig_accuracy(predictions):\n",
    "    return f\"Training Set accuracy: {accuracy_score(Y_train, predictions):.2f}\"\n",
    "\n",
    "\n",
    "def split_features_by_type(X, feature_structure):\n",
    "    \"\"\"\n",
    "    Splits the dataset into subsets based on the feature structure provided.\n",
    "\n",
    "    :param X: numpy array, the dataset to be split (features only)\n",
    "    :param feature_structure: dict, keys are feature names and values are the number of features of that type\n",
    "    :return: dict of feature subsets\n",
    "    \"\"\"\n",
    "    feature_subsets = {}\n",
    "    start_idx = 0\n",
    "    \n",
    "    for feature_name, feature_count in feature_structure.items():\n",
    "        end_idx = start_idx + feature_count\n",
    "        feature_subsets[feature_name] = X[:, start_idx:end_idx]\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    return feature_subsets\n",
    "\n",
    "# Define the structure of your features based on the information you've provided\n",
    "feature_structure = {\n",
    "    'chroma_cens': 84,\n",
    "    'chroma_cqt': 84,\n",
    "    'chroma_stft': 84,\n",
    "    'mfcc': 140,\n",
    "    'rmse': 7,\n",
    "    'spectral_bandwidth': 7,\n",
    "    'spectral_centroid': 7,\n",
    "    'spectral_contrast': 49,\n",
    "    'spectral_rolloff': 7,\n",
    "    'tonnetz': 42,\n",
    "    'zcr': 7\n",
    "}\n",
    "\n",
    "# Example usage with a hypothetical dataset X_train_scaled\n",
    "# This would be your preprocessed and scaled training data as a NumPy array\n",
    "train_feature_subsets = split_features_by_type(X_train_scaled, feature_structure)\n",
    "val_feature_subsets = split_features_by_type(X_val_scaled, feature_structure)\n",
    "test_feature_subsets = split_features_by_type(X_test_scaled, feature_structure)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:43:51.995132Z",
     "start_time": "2024-03-16T17:43:51.531474Z"
    }
   },
   "id": "7fcee7620916ee2f",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set accuracy: 0.52\n",
      "Pseudo Test Set accuracy: 0.40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Number of base estimators\n",
    "n_estimators = 10  # You can experiment with this number\n",
    "\n",
    "# Create a BaggingClassifier with KNN as base estimators\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=KNeighborsClassifier(n_neighbors=1), \n",
    "    n_estimators=n_estimators, \n",
    "    max_samples=1.0 / n_estimators, \n",
    "    bootstrap=True, \n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all cores\n",
    ")\n",
    "\n",
    "# Fit the BaggingClassifier\n",
    "bagging_clf.fit(X_train_scaled, Y_train)\n",
    "# training predictions\n",
    "trainpreds = bagging_clf.predict(X_train_scaled)\n",
    "# test predictions\n",
    "testpreds = bagging_clf.predict(X_test_scaled)\n",
    "\n",
    "print(calculate_trainig_accuracy(trainpreds))\n",
    "print(calculate_test_accuracy(testpreds))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:43:54.017069Z",
     "start_time": "2024-03-16T17:43:52.005094Z"
    }
   },
   "id": "e75f1d68e1c19e64",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression Feature Subset "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55e1cb28a316d859"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a dictionary to store the logistic regression models for each feature subset\n",
    "best_lr_models = {}\n",
    "\n",
    "# Train a logistic regression model for each feature subset\n",
    "for feature_name, X_train_subset in train_feature_subsets.items():\n",
    "    lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs',random_state=42, max_iter=10000)\n",
    "    lr_model.fit(X_train_subset, Y_train)\n",
    "    best_lr_models[feature_name] = lr_model\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:43:57.664666Z",
     "start_time": "2024-03-16T17:43:54.018236Z"
    }
   },
   "id": "808178aa1202d8a7",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "532f83c8df74c40e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.2825\n",
      "Validation accuracy for chroma_cqt features: 0.295\n",
      "Validation accuracy for chroma_stft features: 0.3525\n",
      "Validation accuracy for mfcc features: 0.4716666666666667\n",
      "Validation accuracy for rmse features: 0.26166666666666666\n",
      "Validation accuracy for spectral_bandwidth features: 0.29583333333333334\n",
      "Validation accuracy for spectral_centroid features: 0.31333333333333335\n",
      "Validation accuracy for spectral_contrast features: 0.4083333333333333\n",
      "Validation accuracy for spectral_rolloff features: 0.3125\n",
      "Validation accuracy for tonnetz features: 0.2925\n",
      "Validation accuracy for zcr features: 0.30333333333333334\n",
      "Test accuracy with combined Logistic Regression models using weighted voting: 0.5341666666666667\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "\n",
    "for feature_name, model in best_lr_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy}\")\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# Split the test set using the same feature structure and prepare for weighted predictions\n",
    "weighted_test_predictions = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_lr_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * weights[i]\n",
    "    weighted_test_predictions += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions\n",
    "combined_test_predictions = np.argmax(weighted_test_predictions, axis=1)\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = np.mean(combined_test_predictions == Y_test)\n",
    "print(f\"Test accuracy with combined Logistic Regression models using weighted voting: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:43:57.782167Z",
     "start_time": "2024-03-16T17:43:57.683254Z"
    }
   },
   "id": "9baecaedf94de3e",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Feature Subset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82c753f321ee17a0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the preprocessing and data splitting is already done as per your provided code\n",
    "\n",
    "# Initialize a dictionary to store your best Random Forest models for each feature subset\n",
    "best_rf_models = {}\n",
    "\n",
    "# Train a Random Forest model for each feature subset\n",
    "for feature_name, X_train_subset in train_feature_subsets.items():\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train_subset, Y_train)\n",
    "    best_rf_models[feature_name] = rf_model\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:44:11.378224Z",
     "start_time": "2024-03-16T17:43:57.786915Z"
    }
   },
   "id": "a0e8cf3bafd2d6cc",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caf1e4c99f7ec42f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.3075\n",
      "Validation accuracy for chroma_cqt features: 0.335\n",
      "Validation accuracy for chroma_stft features: 0.38666666666666666\n",
      "Validation accuracy for mfcc features: 0.51\n",
      "Validation accuracy for rmse features: 0.2708333333333333\n",
      "Validation accuracy for spectral_bandwidth features: 0.33\n",
      "Validation accuracy for spectral_centroid features: 0.3591666666666667\n",
      "Validation accuracy for spectral_contrast features: 0.46\n",
      "Validation accuracy for spectral_rolloff features: 0.3458333333333333\n",
      "Validation accuracy for tonnetz features: 0.3\n",
      "Validation accuracy for zcr features: 0.3333333333333333\n",
      "Test accuracy with combined Random Forest models using weighted voting: 0.5375\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "\n",
    "for feature_name, model in best_rf_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy}\")\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# Split the test set using the same feature structure and prepare for weighted predictions\n",
    "weighted_test_predictions = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_rf_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * weights[i]\n",
    "    weighted_test_predictions += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions\n",
    "combined_test_predictions = np.argmax(weighted_test_predictions, axis=1)\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = np.mean(combined_test_predictions == Y_test)\n",
    "print(f\"Test accuracy with combined Random Forest models using weighted voting: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:44:11.820775Z",
     "start_time": "2024-03-16T17:44:11.378952Z"
    }
   },
   "id": "6891a3d5e4eb42e4",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM Subset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca5b13503dbef385"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize a dictionary to store the SVM models for each feature subset\n",
    "best_svm_models = {}\n",
    "\n",
    "# Train a polynomial kernel SVM model for each feature subset\n",
    "for feature_name, X_train_subset in train_feature_subsets.items():\n",
    "    # Specify the polynomial kernel using the `kernel` parameter\n",
    "    svm_model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "    svm_model.fit(X_train_subset, Y_train)\n",
    "    best_svm_models[feature_name] = svm_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:44:44.488003Z",
     "start_time": "2024-03-16T17:44:11.821787Z"
    }
   },
   "id": "37860a9f8c1639da",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ce54890c084b26b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.3333333333333333\n",
      "Validation accuracy for chroma_cqt features: 0.34833333333333333\n",
      "Validation accuracy for chroma_stft features: 0.38333333333333336\n",
      "Validation accuracy for mfcc features: 0.56\n",
      "Validation accuracy for rmse features: 0.26666666666666666\n",
      "Validation accuracy for spectral_bandwidth features: 0.32666666666666666\n",
      "Validation accuracy for spectral_centroid features: 0.3825\n",
      "Validation accuracy for spectral_contrast features: 0.4725\n",
      "Validation accuracy for spectral_rolloff features: 0.37\n",
      "Validation accuracy for tonnetz features: 0.3225\n",
      "Validation accuracy for zcr features: 0.33916666666666667\n",
      "Test accuracy with combined SVM models using weighted voting: 0.5708333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weights = []\n",
    "\n",
    "for feature_name, model in best_svm_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy}\")\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# Split the test set using the same feature structure and prepare for weighted predictions\n",
    "weighted_test_predictions = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_svm_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * weights[i]\n",
    "    weighted_test_predictions += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions\n",
    "combined_test_predictions = np.argmax(weighted_test_predictions, axis=1)\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = np.mean(combined_test_predictions == Y_test)\n",
    "print(f\"Test accuracy with combined SVM models using weighted voting: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:44:50.783597Z",
     "start_time": "2024-03-16T17:44:44.488751Z"
    }
   },
   "id": "366ca8a554ba5e7c",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost Subset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b8ab3aaf2029b7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/sml-practical-env/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Initialize a dictionary to store the XGBoost models for each feature subset\n",
    "best_xgb_models = {}\n",
    "\n",
    "# Train an XGBoost model for each feature subset\n",
    "for feature_name, X_train_subset in train_feature_subsets.items():\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, objective='multi:softprob', num_class=8, eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model.fit(X_train_subset, Y_train)\n",
    "    best_xgb_models[feature_name] = xgb_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:45:17.828586Z",
     "start_time": "2024-03-16T17:44:50.784555Z"
    }
   },
   "id": "cbcf36d11143ceff",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81be2367c9ef24fe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.31333333333333335\n",
      "Validation accuracy for chroma_cqt features: 0.3275\n",
      "Validation accuracy for chroma_stft features: 0.37416666666666665\n",
      "Validation accuracy for mfcc features: 0.5475\n",
      "Validation accuracy for rmse features: 0.25583333333333336\n",
      "Validation accuracy for spectral_bandwidth features: 0.30666666666666664\n",
      "Validation accuracy for spectral_centroid features: 0.3258333333333333\n",
      "Validation accuracy for spectral_contrast features: 0.4666666666666667\n",
      "Validation accuracy for spectral_rolloff features: 0.3408333333333333\n",
      "Validation accuracy for tonnetz features: 0.32666666666666666\n",
      "Validation accuracy for zcr features: 0.3175\n",
      "Test accuracy with combined SVM models using weighted voting: 0.5758333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weights = []\n",
    "\n",
    "for feature_name, model in best_xgb_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy}\")\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# Split the test set using the same feature structure and prepare for weighted predictions\n",
    "weighted_test_predictions = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_xgb_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * weights[i]\n",
    "    weighted_test_predictions += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions\n",
    "combined_test_predictions = np.argmax(weighted_test_predictions, axis=1)\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = np.mean(combined_test_predictions == Y_test)\n",
    "print(f\"Test accuracy with combined SVM models using weighted voting: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:45:17.907375Z",
     "start_time": "2024-03-16T17:45:17.831701Z"
    }
   },
   "id": "c818a3713a71ece9",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KNN Subset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d4bdc5e166a5445"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K for chroma_cens features: 9 with cross-validation score: 0.2663888888888889\n",
      "Best K for chroma_cqt features: 13 with cross-validation score: 0.27361111111111114\n",
      "Best K for chroma_stft features: 13 with cross-validation score: 0.30194444444444446\n",
      "Best K for mfcc features: 15 with cross-validation score: 0.46611111111111103\n",
      "Best K for rmse features: 15 with cross-validation score: 0.23555555555555552\n",
      "Best K for spectral_bandwidth features: 15 with cross-validation score: 0.29083333333333333\n",
      "Best K for spectral_centroid features: 11 with cross-validation score: 0.32166666666666666\n",
      "Best K for spectral_contrast features: 12 with cross-validation score: 0.40861111111111115\n",
      "Best K for spectral_rolloff features: 12 with cross-validation score: 0.3061111111111111\n",
      "Best K for tonnetz features: 6 with cross-validation score: 0.2725000000000001\n",
      "Best K for zcr features: 14 with cross-validation score: 0.30583333333333335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_models = {}\n",
    "\n",
    "# Train a KNN model for each feature subset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Dictionary to store the best KNN models for each feature subset\n",
    "best_knn_models = {}\n",
    "\n",
    "# Train a KNN model for each feature subset and find the best k using cross-validation\n",
    "for feature_name, X_subset in train_feature_subsets.items():\n",
    "    best_score = 0\n",
    "    best_k = 1\n",
    "    # Try different values of k\n",
    "    for k in range(1, 16):  # Let's try k from 1 to 15 as an example\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        scores = cross_val_score(knn, X_subset, Y_train, cv=5)\n",
    "        mean_score = scores.mean()\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "\n",
    "    # Train a new KNN model on the full training set with the best k\n",
    "    best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "    best_knn.fit(X_subset, Y_train)\n",
    "    best_knn_models[feature_name] = best_knn\n",
    "    print(f\"Best K for {feature_name} features: {best_k} with cross-validation score: {best_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:45:26.452632Z",
     "start_time": "2024-03-16T17:45:17.908235Z"
    }
   },
   "id": "46c8c266dd3d2db",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bbf099a80b5bab4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.27\n",
      "Validation accuracy for chroma_cqt features: 0.2658333333333333\n",
      "Validation accuracy for chroma_stft features: 0.325\n",
      "Validation accuracy for mfcc features: 0.4741666666666667\n",
      "Validation accuracy for rmse features: 0.22916666666666666\n",
      "Validation accuracy for spectral_bandwidth features: 0.29583333333333334\n",
      "Validation accuracy for spectral_centroid features: 0.3416666666666667\n",
      "Validation accuracy for spectral_contrast features: 0.41083333333333333\n",
      "Validation accuracy for spectral_rolloff features: 0.3225\n",
      "Validation accuracy for tonnetz features: 0.25\n",
      "Validation accuracy for zcr features: 0.295\n",
      "Test accuracy with combined KNN models using weighted voting: 0.5208333333333334\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "\n",
    "for feature_name, model in best_knn_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy}\")\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# Split the test set using the same feature structure and prepare for weighted predictions\n",
    "weighted_test_predictions = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_knn_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * weights[i]\n",
    "    weighted_test_predictions += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions\n",
    "combined_test_predictions = np.argmax(weighted_test_predictions, axis=1)\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = np.mean(combined_test_predictions == Y_test)\n",
    "print(f\"Test accuracy with combined KNN models using weighted voting: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:45:26.861543Z",
     "start_time": "2024-03-16T17:45:26.453468Z"
    }
   },
   "id": "ab983bc3fefa5af4",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stacked Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85f7943aab994299"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Meta Learner"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8077032641778916"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "meta_model = LogisticRegression(random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:45:26.864357Z",
     "start_time": "2024-03-16T17:45:26.862604Z"
    }
   },
   "id": "c7d098550c750e99",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF and KNN "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df9fa480695fe13c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF and KNN models: 0.5783333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data for the meta-model\n",
    "# Generate predictions from RF and KNN models on the validation set\n",
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        model.predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]  # Add a new axis for stacking\n",
    "        for model in (best_rf_models[feature_name], best_knn_models[feature_name])\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF and KNN models for each feature subset\n",
    "    for feature_name in train_feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Prepare test data in a similar manner\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        model.predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "        for model in (best_rf_models[feature_name], best_knn_models[feature_name])\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in train_feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the meta-model\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF and KNN models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:45:27.770472Z",
     "start_time": "2024-03-16T17:45:26.865165Z"
    }
   },
   "id": "1c50e82e19591c74",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e774966d38aba43b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF, XGB, SVM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea56edceac76c0e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF, SVM, and XGBoost models: 0.6133333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC  # For SVM\n",
    "from xgboost import XGBClassifier  # For XGBoost\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Prepare X_meta_train by averaging predictions from RF, SVM, and XGB models for each feature subset\n",
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_xgb_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF, SVM, and XGB for each feature subset\n",
    "    for feature_name in train_feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)\n",
    "\n",
    "# Similar preparation for X_meta_test\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_xgb_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF, SVM, and XGB for each feature subset\n",
    "    for feature_name in train_feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Assuming meta_model is already defined and ready for training\n",
    "# Train the meta-model on the updated X_meta_train dataset\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Make final predictions with the updated meta-model\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, SVM, and XGBoost models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:45:34.813315Z",
     "start_time": "2024-03-16T17:45:27.773945Z"
    }
   },
   "id": "a05737cca19a7377",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF, KNN, LR, SVM, and XGBoost models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8d4cf55233bcb3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF, KNN, LR, SVM, and XGBoost models: 0.6133333333333333\n"
     ]
    }
   ],
   "source": [
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],  # Existing SVM predictions\n",
    "        best_xgb_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]  # Add XGBoost predictions\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from all models for each feature subset\n",
    "    for feature_name in train_feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "# Re-train the meta-model with the updated training data including XGBoost predictions\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Prepare the test data in a similar manner to include XGBoost predictions\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],  # Existing SVM predictions\n",
    "        best_xgb_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]  # Add XGBoost predictions\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in train_feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the updated meta-model including XGBoost\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of XGBoost models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, LR, SVM, and XGBoost models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T17:45:42.723157Z",
     "start_time": "2024-03-16T17:45:34.819649Z"
    }
   },
   "id": "666689041cfe40b9",
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
