{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup Cell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 different classes: Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop or Rock.\n",
      "objective 1: construct a classifier which, based on the features of a song, predicts its genre\n",
      "objective 2: estimate its generalisation error under the 0â€“1 loss.\n",
      "Features are real-valued, correspond to summary statistics (mean, sd, skewness, kurtosis, median, min, max) of \n",
      "time series of various music features, such as the chromagram or the Mel-frequency cepstrum.\n",
      "Feature description: \n",
      "\n",
      "Feature description: \n",
      "chroma_cens: Chroma Energy Normalized (CENS, 12 chroma) - 84 features\n",
      "chroma_cqt: Constant-Q chromagram (12 chroma) - 84 features\n",
      "chroma_stft: Chromagram (12 chroma) - 84 features\n",
      "mfcc: Mel-frequency cepstrum (20 coefficients) - 140 features\n",
      "rmse: Root-mean-square - 7 features\n",
      "spectral_bandwidth: Spectral bandwidth - 7 features\n",
      "spectral_centroid: Spectral centroid - 7 features\n",
      "spectral_contrast: Spectral contrast (7 frequency bands) - 49 features\n",
      "spectral_rolloff: Roll-off frequency - 7 features\n",
      "tonnetz: Tonal centroid features (6 features) - 42 features\n",
      "zcr: Zero-crossing rate - 7 features\n",
      "x_train: 6000 rows on 518 columns\n",
      "Objects loaded: x_train, x_test, y_train as pd dataframes, x_train_np, x_test_np, y_train_np as NP arrays\n",
      "function generate_submission_csv(genre_predictions, filename='submission.csv') available\n"
     ]
    }
   ],
   "source": [
    "%run 'Setup.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Splitting, Scaling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_np.ravel()) #\n",
    "\n",
    "# Split training data into training and temporary validation sets\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(x_train, y_train_encoded, test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the temporary validation set into validation and test sets\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_real_test_scaled = scaler.transform(x_test_np)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Function Split Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def split_features_by_type(X, feature_structure):\n",
    "    \"\"\"\n",
    "    Splits the dataset into subsets based on the feature structure provided.\n",
    "\n",
    "    :param X: numpy array, the dataset to be split (features only)\n",
    "    :param feature_structure: dict, keys are feature names and values are the number of features of that type\n",
    "    :return: dict of feature subsets\n",
    "    \"\"\"\n",
    "    feature_subsets = {}\n",
    "    start_idx = 0\n",
    "\n",
    "    for feature_name, feature_count in feature_structure.items():\n",
    "        end_idx = start_idx + feature_count\n",
    "        feature_subsets[feature_name] = X[:, start_idx:end_idx]\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return feature_subsets\n",
    "\n",
    "# Define the structure of your features based on the information you've provided\n",
    "feature_structure = {\n",
    "    'chroma_cens': 84,\n",
    "    'chroma_cqt': 84,\n",
    "    'chroma_stft': 84,\n",
    "    'mfcc': 140,\n",
    "    'rmse': 7,\n",
    "    'spectral_bandwidth': 7,\n",
    "    'spectral_centroid': 7,\n",
    "    'spectral_contrast': 49,\n",
    "    'spectral_rolloff': 7,\n",
    "    'tonnetz': 42,\n",
    "    'zcr': 7\n",
    "}\n",
    "\n",
    "# Example usage with a hypothetical dataset X_train_scaled\n",
    "# This would be your preprocessed and scaled training data as a NumPy array\n",
    "feature_subsets = split_features_by_type(X_train_scaled, feature_structure)\n",
    "\n",
    "# Now feature_subsets is a dictionary where, for example,\n",
    "# feature_subsets['mfcc'] contains only the MFCC features of the dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subset Fits for Ensemble Learners"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Feature Subset Fit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the preprocessing and data splitting is already done as per your provided code\n",
    "\n",
    "# Initialize a dictionary to store your best Random Forest models for each feature subset\n",
    "best_rf_models = {}\n",
    "\n",
    "# Train a Random Forest model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train_subset, Y_train)\n",
    "    best_rf_models[feature_name] = rf_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RF Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for Random Forest on chroma_cens...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for chroma_cens: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on chroma_cqt...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for chroma_cqt: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on chroma_stft...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for chroma_stft: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on mfcc...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for mfcc: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on rmse...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for rmse: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on spectral_bandwidth...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for spectral_bandwidth: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Starting GridSearchCV for Random Forest on spectral_centroid...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for spectral_centroid: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on spectral_contrast...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for spectral_contrast: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on spectral_rolloff...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for spectral_rolloff: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Starting GridSearchCV for Random Forest on tonnetz...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for tonnetz: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on zcr...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for zcr: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a dictionary to store your best Random Forest models for each feature subset\n",
    "best_rf_models = {}\n",
    "\n",
    "# Define a parameter grid to search for best parameters for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],  # Reduced from 3 to 2 options\n",
    "    'max_depth': [None, 20],  # Reduced from 4 to 2 options\n",
    "    'min_samples_split': [2, 10],  # Reduced from 3 to 2 options\n",
    "    'min_samples_leaf': [1, 4]  # Reduced from 3 to 2 options\n",
    "}\n",
    "\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    print(f\"Starting GridSearchCV for Random Forest on {feature_name}...\")\n",
    "\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train_subset, Y_train)\n",
    "\n",
    "    best_rf_models[feature_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {feature_name}: {grid_search.best_params_}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.29\n",
      "Validation accuracy for chroma_cqt features: 0.33\n",
      "Validation accuracy for chroma_stft features: 0.3883333333333333\n",
      "Validation accuracy for mfcc features: 0.5133333333333333\n",
      "Validation accuracy for rmse features: 0.26666666666666666\n",
      "Validation accuracy for spectral_bandwidth features: 0.33166666666666667\n",
      "Validation accuracy for spectral_centroid features: 0.3525\n",
      "Validation accuracy for spectral_contrast features: 0.4608333333333333\n",
      "Validation accuracy for spectral_rolloff features: 0.3425\n",
      "Validation accuracy for tonnetz features: 0.2966666666666667\n",
      "Validation accuracy for zcr features: 0.3325\n",
      "Test accuracy with combined Random Forest models using weighted voting: 0.5408333333333334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate each model on the validation set and calculate weights\n",
    "weights = []\n",
    "val_feature_subsets = split_features_by_type(X_val_scaled, feature_structure)\n",
    "\n",
    "for feature_name, model in best_rf_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy}\")\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# Split the test set using the same feature structure and prepare for weighted predictions\n",
    "test_feature_subsets = split_features_by_type(X_test_scaled, feature_structure)\n",
    "weighted_test_predictions = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_rf_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * weights[i]\n",
    "    weighted_test_predictions += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions\n",
    "combined_test_predictions = np.argmax(weighted_test_predictions, axis=1)\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = np.mean(combined_test_predictions == Y_test)\n",
    "print(f\"Test accuracy with combined Random Forest models using weighted voting: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN Feature Subset Fit (from KNN.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K for chroma_cens features: 9 with cross-validation score: 0.2663888888888889\n",
      "Best K for chroma_cqt features: 13 with cross-validation score: 0.27361111111111114\n",
      "Best K for chroma_stft features: 13 with cross-validation score: 0.30194444444444446\n",
      "Best K for mfcc features: 15 with cross-validation score: 0.46611111111111103\n",
      "Best K for rmse features: 15 with cross-validation score: 0.23555555555555552\n",
      "Best K for spectral_bandwidth features: 15 with cross-validation score: 0.29083333333333333\n",
      "Best K for spectral_centroid features: 11 with cross-validation score: 0.32166666666666666\n",
      "Best K for spectral_contrast features: 12 with cross-validation score: 0.40861111111111115\n",
      "Best K for spectral_rolloff features: 12 with cross-validation score: 0.3061111111111111\n",
      "Best K for tonnetz features: 6 with cross-validation score: 0.2725000000000001\n",
      "Best K for zcr features: 14 with cross-validation score: 0.30583333333333335\n",
      "Validation accuracy with combined KNN models: 0.4725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Dictionary to store the trained KNN models for each feature subset\n",
    "knn_models = {}\n",
    "\n",
    "# Train a KNN model for each feature subset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Dictionary to store the best KNN models for each feature subset\n",
    "best_knn_models = {}\n",
    "\n",
    "# Train a KNN model for each feature subset and find the best k using cross-validation\n",
    "for feature_name, X_subset in feature_subsets.items():\n",
    "    best_score = 0\n",
    "    best_k = 1\n",
    "    # Try different values of k\n",
    "    for k in range(1, 16):  # Let's try k from 1 to 15 as an example\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        scores = cross_val_score(knn, X_subset, Y_train, cv=5)\n",
    "        mean_score = scores.mean()\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "\n",
    "    # Train a new KNN model on the full training set with the best k\n",
    "    best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "    best_knn.fit(X_subset, Y_train)\n",
    "    best_knn_models[feature_name] = best_knn\n",
    "    print(f\"Best K for {feature_name} features: {best_k} with cross-validation score: {best_score}\")\n",
    "\n",
    "# Now best_knn_models dictionary contains the best KNN model for each feature subset\n",
    "\n",
    "\n",
    "# Now knn_models dictionary contains a trained KNN model for each feature subset\n",
    "# For example, knn_models['mfcc'] is the KNN model trained on the MFCC features\n",
    "\n",
    "# To make predictions, use the corresponding model for each feature subset\n",
    "# For instance, for MFCC features:\n",
    "# predictions_mfcc = knn_models['mfcc'].predict(feature_subsets['mfcc'])\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Assume we have a validation set X_val_scaled\n",
    "# Split it using the same function we defined earlier\n",
    "val_feature_subsets = split_features_by_type(X_val_scaled, feature_structure)\n",
    "\n",
    "# Gather predictions from all models on the validation set\n",
    "val_predictions = []\n",
    "for feature_name, model in best_knn_models.items():\n",
    "    # Ensure that we predict on the correct feature subset\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    predictions = model.predict(X_val_subset)\n",
    "    val_predictions.append(predictions)\n",
    "\n",
    "# Combine predictions using majority voting\n",
    "combined_val_predictions = mode(val_predictions, axis=0).mode\n",
    "\n",
    "# Calculate accuracy or any other metric based on the combined predictions\n",
    "val_accuracy = np.mean(combined_val_predictions.ravel() == Y_val)\n",
    "print(f\"Validation accuracy with combined KNN models: {val_accuracy}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression Feature Subset Fit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a dictionary to store the logistic regression models for each feature subset\n",
    "best_lr_models = {}\n",
    "\n",
    "# Train a logistic regression model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr_model.fit(X_train_subset, Y_train)\n",
    "    best_lr_models[feature_name] = lr_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM Feature Subset Fit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize a dictionary to store the SVM models for each feature subset\n",
    "best_svm_models = {}\n",
    "\n",
    "# Train a polynomial kernel SVM model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    # Specify the polynomial kernel using the `kernel` parameter\n",
    "    svm_model = SVC(probability=True, kernel='poly', degree=3, random_state=42)\n",
    "    svm_model.fit(X_train_subset, Y_train)\n",
    "    best_svm_models[feature_name] = svm_model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chroma_cens': SVC(probability=True, random_state=42), 'chroma_cqt': SVC(probability=True, random_state=42), 'chroma_stft': SVC(probability=True, random_state=42), 'mfcc': SVC(probability=True, random_state=42), 'rmse': SVC(probability=True, random_state=42), 'spectral_bandwidth': SVC(probability=True, random_state=42), 'spectral_centroid': SVC(probability=True, random_state=42), 'spectral_contrast': SVC(probability=True, random_state=42), 'spectral_rolloff': SVC(probability=True, random_state=42), 'tonnetz': SVC(probability=True, random_state=42), 'zcr': SVC(probability=True, random_state=42)}\n"
     ]
    }
   ],
   "source": [
    "print(best_svm_models)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.33\n",
      "Validation accuracy for chroma_cqt features: 0.35\n",
      "Validation accuracy for chroma_stft features: 0.38\n",
      "Validation accuracy for mfcc features: 0.56\n",
      "Validation accuracy for rmse features: 0.27\n",
      "Validation accuracy for spectral_bandwidth features: 0.33\n",
      "Validation accuracy for spectral_centroid features: 0.38\n",
      "Validation accuracy for spectral_contrast features: 0.47\n",
      "Validation accuracy for spectral_rolloff features: 0.37\n",
      "Validation accuracy for tonnetz features: 0.32\n",
      "Validation accuracy for zcr features: 0.34\n",
      "Test accuracy with combined SVM models using weighted voting: 0.57\n"
     ]
    }
   ],
   "source": [
    "# Calculate the validation accuracy for each feature subset and use it as weight for voting\n",
    "svm_weights = []\n",
    "for feature_name, model in best_svm_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    svm_weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy:.2f}\")\n",
    "\n",
    "# Normalize weights so they sum up to 1\n",
    "svm_weights = np.array(svm_weights) / np.sum(svm_weights)\n",
    "\n",
    "# Predict on the test set with each SVM model and weight the predictions\n",
    "weighted_test_predictions_svm = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)  # Adjust the shape according to your number of classes\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_svm_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * svm_weights[i]\n",
    "    weighted_test_predictions_svm += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions by taking the argmax to get final predictions\n",
    "combined_test_predictions_svm = np.argmax(weighted_test_predictions_svm, axis=1)\n",
    "\n",
    "# Calculate accuracy based on the combined weighted predictions\n",
    "test_accuracy_svm = np.mean(combined_test_predictions_svm == Y_test)\n",
    "print(f\"Test accuracy with combined SVM models using weighted voting: {test_accuracy_svm:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost Feature Subset Fit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Initialize a dictionary to store the XGBoost models for each feature subset\n",
    "best_xgb_models = {}\n",
    "\n",
    "# Train an XGBoost model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, objective='multi:softprob', num_class=8, eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model.fit(X_train_subset, Y_train)\n",
    "    best_xgb_models[feature_name] = xgb_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross Validation Optimal Model for each Subset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for XGBoost on chroma_cens...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for chroma_cens: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Starting GridSearchCV for XGBoost on chroma_cqt...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming X_train, Y_train are defined and feature_subsets is a dictionary with your subsets\n",
    "\n",
    "best_xgb_models = {}\n",
    "\n",
    "# Define a parameter grid to search for best parameters for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    print(f\"Starting GridSearchCV for XGBoost on {feature_name}...\")\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, objective='multi:softprob', num_class=8, eval_metric='mlogloss', random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train_subset, Y_train)\n",
    "\n",
    "    best_xgb_models[feature_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {feature_name}: {grid_search.best_params_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def train_neural_network(X_train, Y_train_encoded):\n",
    "    model = Sequential(name='Feature_Subset_NN')\n",
    "    model.add(Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(Y_train_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Assuming a modest number of epochs for demonstration; adjust as necessary\n",
    "    model.fit(X_train, Y_train_encoded, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "best_nn_models = {}\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    Y_train_encoded = to_categorical(Y_train)\n",
    "    nn_model = train_neural_network(X_train_subset, Y_train_encoded)\n",
    "    best_nn_models[feature_name] = nn_model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stacked Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Meta Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Default Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "meta_model = LogisticRegression(random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LR Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# Train the meta-model\n",
    "meta_model_lr = LogisticRegression(random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Initialize the XGBoost classifier as the meta-model\n",
    "meta_model_xgb = xgb.XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    n_estimators=500,  # Increased from a default value, which might be around 100\n",
    "    learning_rate=0.05,  # Adjust learning rate to control the pace of boosting and prevent overfitting\n",
    "    max_depth=10,  # Consider setting max_depth to regulate tree complexity\n",
    "    objective='multi:softprob',\n",
    "    num_class=8,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest classifier as the meta-model\n",
    "meta_model_rf = RandomForestClassifier(\n",
    "    n_estimators=500,  # Number of trees in the forest\n",
    "    max_depth=10,  # Maximum depth of the trees to limit complexity and prevent overfitting\n",
    "    min_samples_split=2,  # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=1,  # Minimum number of samples required to be at a leaf node\n",
    "    criterion='gini',  # Function to measure the quality of a split. 'entropy' can also be used for information gain\n",
    "    random_state=42,  # Ensures a deterministic outcome for reproducible results\n",
    "    n_jobs=-1  # Use all available cores for faster training\n",
    ")\n",
    "\n",
    "# Note: Adjust the above hyperparameters based on your dataset characteristics and computational resources\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# Initialize the SVM classifier as the meta-model\n",
    "# 'probability=True' enables the predict_proba method, which is useful for getting predicted probabilities\n",
    "meta_model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NN Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert target variable to categorical (one-hot encoding) for neural network\n",
    "Y_val_encoded = to_categorical(Y_val)\n",
    "\n",
    "# Define the neural network architecture with wider and more layers\n",
    "meta_model_nn = Sequential(name='Enhanced_MetaModel_NN')\n",
    "meta_model_nn.add(Dense(128, input_shape=(X_meta_train.shape[1],), activation='relu', name='Dense_Layer_1'))\n",
    "meta_model_nn.add(Dropout(0.2, name='Dropout_1'))  # Add dropout for regularization\n",
    "meta_model_nn.add(Dense(64, activation='relu', name='Dense_Layer_2'))\n",
    "meta_model_nn.add(Dropout(0.2, name='Dropout_2'))  # Add dropout for regularization\n",
    "meta_model_nn.add(Dense(64, activation='relu', name='Dense_Layer_3'))  # Additional layer\n",
    "meta_model_nn.add(Dense(Y_val_encoded.shape[1], activation='softmax', name='Output_Layer'))\n",
    "\n",
    "# Compile the meta-model\n",
    "meta_model_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF and KNN Stacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF and KNN models: 0.5758333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data for the meta-model\n",
    "# Generate predictions from RF and KNN models on the validation set\n",
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        model.predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]  # Add a new axis for stacking\n",
    "        for model in (best_rf_models[feature_name], best_knn_models[feature_name])\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF and KNN models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Prepare test data in a similar manner\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        model.predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "        for model in (best_rf_models[feature_name], best_knn_models[feature_name])\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the meta-model\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF and KNN models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF, KNN, and LR Stacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF, KNN, and LR models: 0.5941666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update the preparation of the data for the meta-model to include logistic regression predictions\n",
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF, KNN, and LR models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "# Train the meta-model with the updated training data\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Update the preparation of the test data in a similar manner to include logistic regression predictions\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the updated meta-model\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of logistic regression models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, and LR models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for chroma_cens: {'C': 10, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for chroma_cqt: {'C': 10, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for chroma_stft: {'C': 10, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['poly'],  # Kernel type\n",
    "    'degree': [2, 3, 4],  # Degree of the polynomial kernel function\n",
    "    'gamma': ['scale', 'auto'],  # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store the best SVM models for each feature subset\n",
    "best_svm_models = {}\n",
    "\n",
    "# Perform grid search to find the best parameters for the SVM model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    # Initialize the SVM model\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "\n",
    "    # Set up the grid search with cross-validation\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train_subset, Y_train)\n",
    "\n",
    "    # Retrieve the best model\n",
    "    best_svm_models[feature_name] = grid_search.best_estimator_\n",
    "\n",
    "    # Optionally, print the best parameters for each feature subset\n",
    "    print(f\"Best parameters for {feature_name}: {grid_search.best_params_}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF, KNN, LR, and SVM models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[124], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Update the preparation of the data for the meta-model to include SVM predictions\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m X_meta_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhstack([\n\u001B[1;32m      3\u001B[0m     np\u001B[38;5;241m.\u001B[39mconcatenate([\n\u001B[1;32m      4\u001B[0m         best_rf_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      5\u001B[0m         best_knn_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      6\u001B[0m         best_lr_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      7\u001B[0m         best_svm_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis]  \u001B[38;5;66;03m# Add SVM predictions\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     ], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mmean(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# Average predictions from RF, KNN, LR, and now SVM models for each feature subset\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature_name \u001B[38;5;129;01min\u001B[39;00m feature_subsets\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[1;32m     10\u001B[0m ])\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;28mlen\u001B[39m(Y_val), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Reshape to have a standard 2D array\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Re-train the meta-model with the updated training data including SVM predictions\u001B[39;00m\n\u001B[1;32m     13\u001B[0m meta_model\u001B[38;5;241m.\u001B[39mfit(X_meta_train, Y_val)\n",
      "Cell \u001B[0;32mIn[124], line 7\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Update the preparation of the data for the meta-model to include SVM predictions\u001B[39;00m\n\u001B[1;32m      2\u001B[0m X_meta_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhstack([\n\u001B[1;32m      3\u001B[0m     np\u001B[38;5;241m.\u001B[39mconcatenate([\n\u001B[1;32m      4\u001B[0m         best_rf_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      5\u001B[0m         best_knn_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      6\u001B[0m         best_lr_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[0;32m----> 7\u001B[0m         \u001B[43mbest_svm_models\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeature_name\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_feature_subsets\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeature_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis]  \u001B[38;5;66;03m# Add SVM predictions\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     ], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mmean(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# Average predictions from RF, KNN, LR, and now SVM models for each feature subset\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature_name \u001B[38;5;129;01min\u001B[39;00m feature_subsets\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[1;32m     10\u001B[0m ])\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;28mlen\u001B[39m(Y_val), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Reshape to have a standard 2D array\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Re-train the meta-model with the updated training data including SVM predictions\u001B[39;00m\n\u001B[1;32m     13\u001B[0m meta_model\u001B[38;5;241m.\u001B[39mfit(X_meta_train, Y_val)\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/svm/_base.py:871\u001B[0m, in \u001B[0;36mBaseSVC.predict_proba\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    865\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NotFittedError(\n\u001B[1;32m    866\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredict_proba is not available when fitted with probability=False\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    867\u001B[0m     )\n\u001B[1;32m    868\u001B[0m pred_proba \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sparse_predict_proba \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sparse \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dense_predict_proba\n\u001B[1;32m    870\u001B[0m )\n\u001B[0;32m--> 871\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpred_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/svm/_base.py:911\u001B[0m, in \u001B[0;36mBaseSVC._dense_predict_proba\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    908\u001B[0m     kernel \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprecomputed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    910\u001B[0m svm_type \u001B[38;5;241m=\u001B[39m LIBSVM_IMPL\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_impl)\n\u001B[0;32m--> 911\u001B[0m pprob \u001B[38;5;241m=\u001B[39m \u001B[43mlibsvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    912\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    913\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msupport_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    914\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msupport_vectors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_n_support\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dual_coef_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_intercept_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_probA\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_probB\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[43m    \u001B[49m\u001B[43msvm_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msvm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    921\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkernel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkernel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    922\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdegree\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdegree\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    923\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef0\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoef0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pprob\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Update the preparation of the data for the meta-model to include SVM predictions\n",
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]  # Add SVM predictions\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF, KNN, LR, and now SVM models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "# Re-train the meta-model with the updated training data including SVM predictions\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Prepare the test data in a similar manner to include SVM predictions\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]  # Add SVM predictions\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the updated meta-model including SVM\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of SVM models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, LR, and SVM models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF, KNN, LR, SVM, and XGBoost models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF, KNN, LR, SVM, and XGBoost models: 0.6116666666666667\n"
     ]
    }
   ],
   "source": [
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],  # Existing SVM predictions\n",
    "        best_xgb_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]  # Add XGBoost predictions\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from all models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "# Re-train the meta-model with the updated training data including XGBoost predictions\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Prepare the test data in a similar manner to include XGBoost predictions\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],  # Existing SVM predictions\n",
    "        best_xgb_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]  # Add XGBoost predictions\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the updated meta-model including XGBoost\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of XGBoost models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, LR, SVM, and XGBoost models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Previous with NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 5ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 1s 7ms/step\n",
      "Test accuracy with stacked RF, KNN, LR, SVM, XGBoost, and Neural Network models: 0.6166666666666667\n"
     ]
    }
   ],
   "source": [
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_xgb_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        # Generate predictions from the NN model for the validation set and add them\n",
    "        best_nn_models[feature_name].predict(val_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)\n",
    "\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Modification for X_meta_test to include neural network predictions\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_xgb_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        # Add neural network predictions for the test set\n",
    "        best_nn_models[feature_name].predict(test_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from all models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the updated meta-model including all models\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of all models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, LR, SVM, XGBoost, and Neural Network models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Meta Learners"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log Reg Meta learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meta_model_lr.fit(X_meta_train, Y_val)\n",
    "# Make final predictions with the updated meta-model including all models\n",
    "final_predictions = meta_model_lr.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of all models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, LR, SVM, XGBoost, and Neural Network models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Meta Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with Random Forest as meta-model: 0.5658333333333333\n"
     ]
    }
   ],
   "source": [
    "# Train the meta-model on the meta-training dataset\n",
    "meta_model_rf.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Make predictions on the meta-test dataset\n",
    "final_predictions_rf = meta_model_rf.predict(X_meta_test)\n",
    "\n",
    "# Evaluate and print the test accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy_rf = accuracy_score(Y_test, final_predictions_rf)\n",
    "print(f\"Test accuracy with Random Forest as meta-model: {test_accuracy_rf}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost Meta Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with Random Forest as meta-model: 0.5658333333333333\n"
     ]
    }
   ],
   "source": [
    "# Train the meta-model on the meta-training dataset\n",
    "meta_model_xgb.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Make predictions on the meta-test dataset\n",
    "final_predictions_xgb = meta_model_xgb.predict(X_meta_test)\n",
    "\n",
    "# Evaluate and print the test accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy_xgb = accuracy_score(Y_test, final_predictions_rf)\n",
    "print(f\"Test accuracy with Random Forest as meta-model: {test_accuracy_rf}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NN as Meta Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4736 - accuracy: 0.8396 - val_loss: 2.0629 - val_accuracy: 0.5125\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4530 - accuracy: 0.8469 - val_loss: 2.0073 - val_accuracy: 0.5125\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3802 - accuracy: 0.8552 - val_loss: 2.0673 - val_accuracy: 0.5667\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3721 - accuracy: 0.8646 - val_loss: 2.0467 - val_accuracy: 0.5667\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3069 - accuracy: 0.8927 - val_loss: 1.9803 - val_accuracy: 0.5542\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2978 - accuracy: 0.8958 - val_loss: 2.0336 - val_accuracy: 0.5458\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2934 - accuracy: 0.8896 - val_loss: 2.1517 - val_accuracy: 0.5333\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2976 - accuracy: 0.8833 - val_loss: 2.1511 - val_accuracy: 0.5292\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2756 - accuracy: 0.8990 - val_loss: 2.1267 - val_accuracy: 0.5542\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2523 - accuracy: 0.9062 - val_loss: 2.1667 - val_accuracy: 0.5417\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2273 - accuracy: 0.9323 - val_loss: 2.2142 - val_accuracy: 0.5500\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2205 - accuracy: 0.9219 - val_loss: 2.3175 - val_accuracy: 0.5292\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2230 - accuracy: 0.9187 - val_loss: 2.1395 - val_accuracy: 0.5542\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2252 - accuracy: 0.9240 - val_loss: 2.3148 - val_accuracy: 0.5458\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2129 - accuracy: 0.9271 - val_loss: 2.2560 - val_accuracy: 0.5458\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the meta-model with early stopping\n",
    "history = meta_model_nn.fit(X_meta_train, Y_val_encoded, epochs=100, batch_size=32,\n",
    "                            validation_split=0.2, verbose=1, callbacks=[early_stopping])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 2.3029 - accuracy: 0.5342\n",
      "Test Loss: 2.302931785583496\n",
      "Test Accuracy: 0.534166693687439\n"
     ]
    }
   ],
   "source": [
    "Y_test_encoded = to_categorical(Y_test)\n",
    "test_loss, test_accuracy = meta_model_nn.evaluate(X_meta_test, Y_test_encoded, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Stacked Bootstrap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with bootstrapped and stacked RF, KNN, and LR models: 0.5933333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Define the number of bootstrap samples and models\n",
    "n_bootstrap_samples = 10\n",
    "\n",
    "# Placeholder for trained bootstrap models\n",
    "bootstrap_models_rf = {feature_name: [] for feature_name in feature_subsets.keys()}\n",
    "bootstrap_models_knn = {feature_name: [] for feature_name in feature_subsets.keys()}\n",
    "bootstrap_models_lr = {feature_name: [] for feature_name in feature_subsets.keys()}\n",
    "\n",
    "# Train bootstrap models for RandomForest\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    for _ in range(n_bootstrap_samples):\n",
    "        # Create a bootstrap sample\n",
    "        X_boot, Y_boot = resample(X_train_subset, Y_train)\n",
    "        # Initialize and train the model on the bootstrap sample\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_boot, Y_boot)\n",
    "        bootstrap_models_rf[feature_name].append(model)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Train bootstrap models for KNN\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    for _ in range(n_bootstrap_samples):\n",
    "        X_boot, Y_boot = resample(X_train_subset, Y_train)\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        knn_model.fit(X_boot, Y_boot)\n",
    "        bootstrap_models_knn[feature_name].append(knn_model)\n",
    "\n",
    "# Train bootstrap models for Logistic Regression\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    for _ in range(n_bootstrap_samples):\n",
    "        X_boot, Y_boot = resample(X_train_subset, Y_train)\n",
    "        lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        lr_model.fit(X_boot, Y_boot)\n",
    "        bootstrap_models_lr[feature_name].append(lr_model)\n",
    "\n",
    "# Generate averaged predictions for the validation set\n",
    "# This assumes `val_feature_subsets` is already prepared similarly to `feature_subsets`\n",
    "X_meta_train = np.hstack([\n",
    "    np.mean([\n",
    "        np.mean([model.predict_proba(val_feature_subsets[feature_name]) for model in models], axis=0)\n",
    "        for models in (bootstrap_models_rf[feature_name], bootstrap_models_knn[feature_name], bootstrap_models_lr[feature_name])\n",
    "    ], axis=0)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)\n",
    "\n",
    "# Train the meta-model\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Assuming `test_feature_subsets` is prepared, generate predictions for the test set\n",
    "X_meta_test = np.hstack([\n",
    "    np.mean([\n",
    "        np.mean([model.predict_proba(test_feature_subsets[feature_name]) for model in models], axis=0)\n",
    "        for models in (bootstrap_models_rf[feature_name], bootstrap_models_knn[feature_name], bootstrap_models_lr[feature_name])\n",
    "    ], axis=0)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Final predictions with the meta-model\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with bootstrapped and stacked RF, KNN, and LR models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Best Model per Feature Subset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with the best model from each feature subset ensemble: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `feature_subsets`, `X_val_scaled`, and `Y_val` are already defined\n",
    "\n",
    "# Step 1: Train models on each feature subset\n",
    "model_types = ['KNN', 'RF', 'LR', 'SVM']\n",
    "best_models = {}\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    # Track the best model for this subset\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "\n",
    "    # KNN Model\n",
    "    knn_model = KNeighborsClassifier()\n",
    "    knn_model.fit(X_train_subset, Y_train)\n",
    "    knn_accuracy = knn_model.score(X_val_subset, Y_val)\n",
    "\n",
    "    if knn_accuracy > best_accuracy:\n",
    "        best_accuracy = knn_accuracy\n",
    "        best_model = ('KNN', knn_model)\n",
    "\n",
    "    # RF Model\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train_subset, Y_train)\n",
    "    rf_accuracy = rf_model.score(X_val_subset, Y_val)\n",
    "\n",
    "    if rf_accuracy > best_accuracy:\n",
    "        best_accuracy = rf_accuracy\n",
    "        best_model = ('RF', rf_model)\n",
    "\n",
    "    # LR Model\n",
    "    lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr_model.fit(X_train_subset, Y_train)\n",
    "    lr_accuracy = lr_model.score(X_val_subset, Y_val)\n",
    "\n",
    "    if lr_accuracy > best_accuracy:\n",
    "        best_accuracy = lr_accuracy\n",
    "        best_model = ('LR', lr_model)\n",
    "\n",
    "# Step 2: Evaluate and select the best model per subset (already done in the loop)\n",
    "\n",
    "# Step 3: Ensemble selected models for final prediction\n",
    "# Generate predictions for each selected model on the test set\n",
    "ensemble_predictions = np.zeros((len(Y_test), len(np.unique(Y_train))))\n",
    "\n",
    "for feature_name, (model_type, model) in best_models.items():\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    ensemble_predictions += predictions\n",
    "\n",
    "# Final ensemble prediction\n",
    "final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with the best model from each feature subset ensemble: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with weighted averaging ensemble: 0.5408333333333334\n"
     ]
    }
   ],
   "source": [
    "# Assuming `best_models` and their validation accuracies are already calculated\n",
    "\n",
    "# Placeholder for models, their weights, and validation feature subsets\n",
    "selected_models = []\n",
    "model_weights = []\n",
    "val_feature_subsets = split_features_by_type(X_val_scaled, feature_structure)  # Ensure this is defined\n",
    "\n",
    "# Calculate the validation accuracy for each selected model and use it as the weight\n",
    "for feature_name, (model_type, model) in best_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    selected_models.append(model)\n",
    "    model_weights.append(val_accuracy)\n",
    "\n",
    "# Normalize the weights so they sum up to 1\n",
    "total_weight = sum(model_weights)\n",
    "normalized_weights = [weight / total_weight for weight in model_weights]\n",
    "\n",
    "# Apply weighted averaging for the final ensemble predictions\n",
    "weighted_ensemble_predictions = np.zeros((len(Y_test), len(np.unique(Y_train))))\n",
    "\n",
    "for model, weight, feature_name in zip(selected_models, normalized_weights, best_models.keys()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    prob_predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_ensemble_predictions += prob_predictions * weight\n",
    "\n",
    "# The final prediction is the class with the highest weighted sum across all models\n",
    "final_predictions_weighted = np.argmax(weighted_ensemble_predictions, axis=1)\n",
    "\n",
    "# Calculate and print the test accuracy with weighted averaging\n",
    "test_accuracy_weighted = accuracy_score(Y_test, final_predictions_weighted)\n",
    "print(f\"Test accuracy with weighted averaging ensemble: {test_accuracy_weighted}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with the best model from each feature subset ensemble: 0.565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC  # For SVM with RBF kernel\n",
    "from xgboost import XGBClassifier  # For XGBoost\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    # Track the best model for this subset\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "\n",
    "    # Define your models here, including SVM and XGBoost\n",
    "    models = {\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'RF': RandomForestClassifier(random_state=42),\n",
    "        'LR': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'SVM': SVC(probability=True, kernel='rbf', random_state=42),  # SVM with RBF kernel\n",
    "        'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)  # XGBoost\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train_subset, Y_train)\n",
    "        accuracy = model.score(X_val_subset, Y_val)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = (model_name, model)\n",
    "\n",
    "    best_models[feature_name] = best_model\n",
    "\n",
    "# Ensemble selected models for final prediction\n",
    "ensemble_predictions = np.zeros((len(Y_test), len(np.unique(Y_train))))\n",
    "\n",
    "for feature_name, (model_type, model) in best_models.items():\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        predictions = model.predict_proba(X_test_subset)\n",
    "    else:  # For models like SVM that might not support predict_proba by default\n",
    "        predictions = model.decision_function(X_test_subset)\n",
    "        predictions = (predictions - predictions.min()) / (predictions.max() - predictions.min())  # Simple scaling to [0,1]\n",
    "    ensemble_predictions += predictions\n",
    "\n",
    "# Final ensemble prediction\n",
    "final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with the best model from each feature subset ensemble: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with weighted ensemble: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty dictionary to store validation accuracies\n",
    "validation_accuracies = {}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    # Track the best model for this subset\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "\n",
    "    # Define your models here, including SVM and XGBoost\n",
    "    models = {\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'RF': RandomForestClassifier(random_state=42),\n",
    "        'LR': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'SVM': SVC(probability=True, kernel='rbf', random_state=42),  # SVM with RBF kernel\n",
    "        'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)  # XGBoost\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train_subset, Y_train)\n",
    "        accuracy = model.score(X_val_subset, Y_val)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = (model_name, model)\n",
    "\n",
    "    # Store the best model and its validation accuracy\n",
    "    best_models[feature_name] = best_model\n",
    "    validation_accuracies[feature_name] = best_accuracy\n",
    "\n",
    "# Ensemble selected models for final prediction with weighted predictions\n",
    "ensemble_predictions = np.zeros((len(Y_test), len(np.unique(Y_train))))\n",
    "\n",
    "for feature_name, (model_type, model) in best_models.items():\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    weight = validation_accuracies[feature_name]\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        predictions = model.predict_proba(X_test_subset) * weight\n",
    "    else:  # For models like SVM that might not support predict_proba by default\n",
    "        # Normalize decision_function output to [0,1] and apply weight\n",
    "        decision_values = model.decision_function(X_test_subset)\n",
    "        predictions = (decision_values - decision_values.min()) / (decision_values.max() - decision_values.min())\n",
    "        predictions *= weight\n",
    "\n",
    "    # Sum the weighted predictions\n",
    "    ensemble_predictions += predictions\n",
    "\n",
    "# Normalize the ensemble predictions to ensure they form a valid probability distribution\n",
    "ensemble_predictions /= ensemble_predictions.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Make final predictions based on the weighted ensemble\n",
    "final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with weighted ensemble: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
