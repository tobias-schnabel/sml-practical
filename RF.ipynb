{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup Cell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 different classes: Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop or Rock.\n",
      "objective 1: construct a classifier which, based on the features of a song, predicts its genre\n",
      "objective 2: estimate its generalisation error under the 0â€“1 loss.\n",
      "Features are real-valued, correspond to summary statistics (mean, sd, skewness, kurtosis, median, min, max) of \n",
      "time series of various music features, such as the chromagram or the Mel-frequency cepstrum.\n",
      "Feature description: \n",
      "\n",
      "Feature description: \n",
      "chroma_cens: Chroma Energy Normalized (CENS, 12 chroma) - 84 features\n",
      "chroma_cqt: Constant-Q chromagram (12 chroma) - 84 features\n",
      "chroma_stft: Chromagram (12 chroma) - 84 features\n",
      "mfcc: Mel-frequency cepstrum (20 coefficients) - 140 features\n",
      "rmse: Root-mean-square - 7 features\n",
      "spectral_bandwidth: Spectral bandwidth - 7 features\n",
      "spectral_centroid: Spectral centroid - 7 features\n",
      "spectral_contrast: Spectral contrast (7 frequency bands) - 49 features\n",
      "spectral_rolloff: Roll-off frequency - 7 features\n",
      "tonnetz: Tonal centroid features (6 features) - 42 features\n",
      "zcr: Zero-crossing rate - 7 features\n",
      "x_train: 6000 rows on 518 columns\n",
      "Objects loaded: x_train, x_test, y_train as pd dataframes, x_train_np, x_test_np, y_train_np as NP arrays\n",
      "function generate_submission_csv(genre_predictions, filename='submission.csv') available\n"
     ]
    }
   ],
   "source": [
    "%run 'Setup.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Splitting, Scaling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_np.ravel()) #\n",
    "\n",
    "# Split training data into training and temporary validation sets\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(x_train, y_train_encoded, test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the temporary validation set into validation and test sets\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_real_test_scaled = scaler.transform(x_test_np)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Function Split Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def split_features_by_type(X, feature_structure):\n",
    "    \"\"\"\n",
    "    Splits the dataset into subsets based on the feature structure provided.\n",
    "\n",
    "    :param X: numpy array, the dataset to be split (features only)\n",
    "    :param feature_structure: dict, keys are feature names and values are the number of features of that type\n",
    "    :return: dict of feature subsets\n",
    "    \"\"\"\n",
    "    feature_subsets = {}\n",
    "    start_idx = 0\n",
    "\n",
    "    for feature_name, feature_count in feature_structure.items():\n",
    "        end_idx = start_idx + feature_count\n",
    "        feature_subsets[feature_name] = X[:, start_idx:end_idx]\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return feature_subsets\n",
    "\n",
    "# Define the structure of your features based on the information you've provided\n",
    "feature_structure = {\n",
    "    'chroma_cens': 84,\n",
    "    'chroma_cqt': 84,\n",
    "    'chroma_stft': 84,\n",
    "    'mfcc': 140,\n",
    "    'rmse': 7,\n",
    "    'spectral_bandwidth': 7,\n",
    "    'spectral_centroid': 7,\n",
    "    'spectral_contrast': 49,\n",
    "    'spectral_rolloff': 7,\n",
    "    'tonnetz': 42,\n",
    "    'zcr': 7\n",
    "}\n",
    "\n",
    "# Example usage with a hypothetical dataset X_train_scaled\n",
    "# This would be your preprocessed and scaled training data as a NumPy array\n",
    "feature_subsets = split_features_by_type(X_train_scaled, feature_structure)\n",
    "\n",
    "# Now feature_subsets is a dictionary where, for example,\n",
    "# feature_subsets['mfcc'] contains only the MFCC features of the dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subset Fits for Ensemble Learners"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Feature Subset Fit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the preprocessing and data splitting is already done as per your provided code\n",
    "\n",
    "# Initialize a dictionary to store your best Random Forest models for each feature subset\n",
    "best_rf_models = {}\n",
    "\n",
    "# Train a Random Forest model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train_subset, Y_train)\n",
    "    best_rf_models[feature_name] = rf_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RF Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for Random Forest on chroma_cens...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for chroma_cens: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on chroma_cqt...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for chroma_cqt: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on chroma_stft...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for chroma_stft: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on mfcc...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for mfcc: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on rmse...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for rmse: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on spectral_bandwidth...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for spectral_bandwidth: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Starting GridSearchCV for Random Forest on spectral_centroid...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for spectral_centroid: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on spectral_contrast...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for spectral_contrast: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on spectral_rolloff...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for spectral_rolloff: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Starting GridSearchCV for Random Forest on tonnetz...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for tonnetz: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Starting GridSearchCV for Random Forest on zcr...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for zcr: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a dictionary to store your best Random Forest models for each feature subset\n",
    "best_rf_models = {}\n",
    "\n",
    "# Define a parameter grid to search for best parameters for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],  # Reduced from 3 to 2 options\n",
    "    'max_depth': [None, 20],  # Reduced from 4 to 2 options\n",
    "    'min_samples_split': [2, 10],  # Reduced from 3 to 2 options\n",
    "    'min_samples_leaf': [1, 4]  # Reduced from 3 to 2 options\n",
    "}\n",
    "\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    print(f\"Starting GridSearchCV for Random Forest on {feature_name}...\")\n",
    "\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train_subset, Y_train)\n",
    "\n",
    "    best_rf_models[feature_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {feature_name}: {grid_search.best_params_}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.29\n",
      "Validation accuracy for chroma_cqt features: 0.33\n",
      "Validation accuracy for chroma_stft features: 0.3883333333333333\n",
      "Validation accuracy for mfcc features: 0.5133333333333333\n",
      "Validation accuracy for rmse features: 0.26666666666666666\n",
      "Validation accuracy for spectral_bandwidth features: 0.33166666666666667\n",
      "Validation accuracy for spectral_centroid features: 0.3525\n",
      "Validation accuracy for spectral_contrast features: 0.4608333333333333\n",
      "Validation accuracy for spectral_rolloff features: 0.3425\n",
      "Validation accuracy for tonnetz features: 0.2966666666666667\n",
      "Validation accuracy for zcr features: 0.3325\n",
      "Test accuracy with combined Random Forest models using weighted voting: 0.5408333333333334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate each model on the validation set and calculate weights\n",
    "weights = []\n",
    "val_feature_subsets = split_features_by_type(X_val_scaled, feature_structure)\n",
    "\n",
    "for feature_name, model in best_rf_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy}\")\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# Split the test set using the same feature structure and prepare for weighted predictions\n",
    "test_feature_subsets = split_features_by_type(X_test_scaled, feature_structure)\n",
    "weighted_test_predictions = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_rf_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * weights[i]\n",
    "    weighted_test_predictions += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions\n",
    "combined_test_predictions = np.argmax(weighted_test_predictions, axis=1)\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = np.mean(combined_test_predictions == Y_test)\n",
    "print(f\"Test accuracy with combined Random Forest models using weighted voting: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN Feature Subset Fit (from KNN.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K for chroma_cens features: 9 with cross-validation score: 0.2663888888888889\n",
      "Best K for chroma_cqt features: 13 with cross-validation score: 0.27361111111111114\n",
      "Best K for chroma_stft features: 13 with cross-validation score: 0.30194444444444446\n",
      "Best K for mfcc features: 15 with cross-validation score: 0.46611111111111103\n",
      "Best K for rmse features: 15 with cross-validation score: 0.23555555555555552\n",
      "Best K for spectral_bandwidth features: 15 with cross-validation score: 0.29083333333333333\n",
      "Best K for spectral_centroid features: 11 with cross-validation score: 0.32166666666666666\n",
      "Best K for spectral_contrast features: 12 with cross-validation score: 0.40861111111111115\n",
      "Best K for spectral_rolloff features: 12 with cross-validation score: 0.3061111111111111\n",
      "Best K for tonnetz features: 6 with cross-validation score: 0.2725000000000001\n",
      "Best K for zcr features: 14 with cross-validation score: 0.30583333333333335\n",
      "Validation accuracy with combined KNN models: 0.4725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Dictionary to store the trained KNN models for each feature subset\n",
    "knn_models = {}\n",
    "\n",
    "# Train a KNN model for each feature subset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Dictionary to store the best KNN models for each feature subset\n",
    "best_knn_models = {}\n",
    "\n",
    "# Train a KNN model for each feature subset and find the best k using cross-validation\n",
    "for feature_name, X_subset in feature_subsets.items():\n",
    "    best_score = 0\n",
    "    best_k = 1\n",
    "    # Try different values of k\n",
    "    for k in range(1, 16):  # Let's try k from 1 to 15 as an example\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        scores = cross_val_score(knn, X_subset, Y_train, cv=5)\n",
    "        mean_score = scores.mean()\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "\n",
    "    # Train a new KNN model on the full training set with the best k\n",
    "    best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "    best_knn.fit(X_subset, Y_train)\n",
    "    best_knn_models[feature_name] = best_knn\n",
    "    print(f\"Best K for {feature_name} features: {best_k} with cross-validation score: {best_score}\")\n",
    "\n",
    "# Now best_knn_models dictionary contains the best KNN model for each feature subset\n",
    "\n",
    "\n",
    "# Now knn_models dictionary contains a trained KNN model for each feature subset\n",
    "# For example, knn_models['mfcc'] is the KNN model trained on the MFCC features\n",
    "\n",
    "# To make predictions, use the corresponding model for each feature subset\n",
    "# For instance, for MFCC features:\n",
    "# predictions_mfcc = knn_models['mfcc'].predict(feature_subsets['mfcc'])\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Assume we have a validation set X_val_scaled\n",
    "# Split it using the same function we defined earlier\n",
    "val_feature_subsets = split_features_by_type(X_val_scaled, feature_structure)\n",
    "\n",
    "# Gather predictions from all models on the validation set\n",
    "val_predictions = []\n",
    "for feature_name, model in best_knn_models.items():\n",
    "    # Ensure that we predict on the correct feature subset\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    predictions = model.predict(X_val_subset)\n",
    "    val_predictions.append(predictions)\n",
    "\n",
    "# Combine predictions using majority voting\n",
    "combined_val_predictions = mode(val_predictions, axis=0).mode\n",
    "\n",
    "# Calculate accuracy or any other metric based on the combined predictions\n",
    "val_accuracy = np.mean(combined_val_predictions.ravel() == Y_val)\n",
    "print(f\"Validation accuracy with combined KNN models: {val_accuracy}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression Feature Subset Fit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a dictionary to store the logistic regression models for each feature subset\n",
    "best_lr_models = {}\n",
    "\n",
    "# Train a logistic regression model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr_model.fit(X_train_subset, Y_train)\n",
    "    best_lr_models[feature_name] = lr_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM Feature Subset Fit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize a dictionary to store the SVM models for each feature subset\n",
    "best_svm_models = {}\n",
    "\n",
    "# Train a polynomial kernel SVM model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    # Specify the polynomial kernel using the `kernel` parameter\n",
    "    svm_model = SVC(probability=True, kernel='poly', degree=3, random_state=42)\n",
    "    svm_model.fit(X_train_subset, Y_train)\n",
    "    best_svm_models[feature_name] = svm_model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chroma_cens': SVC(probability=True, random_state=42), 'chroma_cqt': SVC(probability=True, random_state=42), 'chroma_stft': SVC(probability=True, random_state=42), 'mfcc': SVC(probability=True, random_state=42), 'rmse': SVC(probability=True, random_state=42), 'spectral_bandwidth': SVC(probability=True, random_state=42), 'spectral_centroid': SVC(probability=True, random_state=42), 'spectral_contrast': SVC(probability=True, random_state=42), 'spectral_rolloff': SVC(probability=True, random_state=42), 'tonnetz': SVC(probability=True, random_state=42), 'zcr': SVC(probability=True, random_state=42)}\n"
     ]
    }
   ],
   "source": [
    "print(best_svm_models)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for chroma_cens features: 0.33\n",
      "Validation accuracy for chroma_cqt features: 0.35\n",
      "Validation accuracy for chroma_stft features: 0.38\n",
      "Validation accuracy for mfcc features: 0.56\n",
      "Validation accuracy for rmse features: 0.27\n",
      "Validation accuracy for spectral_bandwidth features: 0.33\n",
      "Validation accuracy for spectral_centroid features: 0.38\n",
      "Validation accuracy for spectral_contrast features: 0.47\n",
      "Validation accuracy for spectral_rolloff features: 0.37\n",
      "Validation accuracy for tonnetz features: 0.32\n",
      "Validation accuracy for zcr features: 0.34\n",
      "Test accuracy with combined SVM models using weighted voting: 0.57\n"
     ]
    }
   ],
   "source": [
    "# Calculate the validation accuracy for each feature subset and use it as weight for voting\n",
    "svm_weights = []\n",
    "for feature_name, model in best_svm_models.items():\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    val_accuracy = model.score(X_val_subset, Y_val)\n",
    "    svm_weights.append(val_accuracy)\n",
    "    print(f\"Validation accuracy for {feature_name} features: {val_accuracy:.2f}\")\n",
    "\n",
    "# Normalize weights so they sum up to 1\n",
    "svm_weights = np.array(svm_weights) / np.sum(svm_weights)\n",
    "\n",
    "# Predict on the test set with each SVM model and weight the predictions\n",
    "weighted_test_predictions_svm = np.zeros((X_test_scaled.shape[0], len(np.unique(Y_train))), dtype=float)  # Adjust the shape according to your number of classes\n",
    "\n",
    "for i, (feature_name, model) in enumerate(best_svm_models.items()):\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    predictions = model.predict_proba(X_test_subset)\n",
    "    weighted_predictions = predictions * svm_weights[i]\n",
    "    weighted_test_predictions_svm += weighted_predictions\n",
    "\n",
    "# Combine weighted predictions by taking the argmax to get final predictions\n",
    "combined_test_predictions_svm = np.argmax(weighted_test_predictions_svm, axis=1)\n",
    "\n",
    "# Calculate accuracy based on the combined weighted predictions\n",
    "test_accuracy_svm = np.mean(combined_test_predictions_svm == Y_test)\n",
    "print(f\"Test accuracy with combined SVM models using weighted voting: {test_accuracy_svm:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost Feature Subset Fit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Initialize a dictionary to store the XGBoost models for each feature subset\n",
    "best_xgb_models = {}\n",
    "\n",
    "# Train an XGBoost model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, objective='multi:softprob', num_class=8, eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model.fit(X_train_subset, Y_train)\n",
    "    best_xgb_models[feature_name] = xgb_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross Validation Optimal Model for each Subset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for XGBoost on chroma_cens...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for chroma_cens: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Starting GridSearchCV for XGBoost on chroma_cqt...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Best parameters for chroma_cqt: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Starting GridSearchCV for XGBoost on chroma_stft...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[131], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m xgb_model \u001B[38;5;241m=\u001B[39m xgb\u001B[38;5;241m.\u001B[39mXGBClassifier(use_label_encoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, objective\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmulti:softprob\u001B[39m\u001B[38;5;124m'\u001B[39m, num_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, eval_metric\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmlogloss\u001B[39m\u001B[38;5;124m'\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m     20\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(estimator\u001B[38;5;241m=\u001B[39mxgb_model, param_grid\u001B[38;5;241m=\u001B[39mxgb_param_grid, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m \u001B[43mgrid_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_subset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m best_xgb_models[feature_name] \u001B[38;5;241m=\u001B[39m grid_search\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest parameters for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfeature_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrid_search\u001B[38;5;241m.\u001B[39mbest_params_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    868\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m    869\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m    870\u001B[0m     )\n\u001B[1;32m    872\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m--> 874\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m    877\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m    878\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1386\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1387\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1388\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    813\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    814\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    816\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    817\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    818\u001B[0m         )\n\u001B[1;32m    819\u001B[0m     )\n\u001B[0;32m--> 821\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    823\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    828\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    833\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    834\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    835\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    836\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    838\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    839\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    840\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    841\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    842\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    843\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     58\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     59\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     60\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     62\u001B[0m )\n\u001B[0;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/joblib/parallel.py:1088\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1085\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[1;32m   1086\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1088\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1089\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m   1091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1092\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[1;32m   1093\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[1;32m   1094\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/joblib/parallel.py:901\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    900\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 901\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    902\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/joblib/parallel.py:819\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    818\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[0;32m--> 819\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    820\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[1;32m    821\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[1;32m    822\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[1;32m    823\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[1;32m    824\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[0;34m(self, func, callback)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    207\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[0;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[1;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    594\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[1;32m    595\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[1;32m    596\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[0;32m--> 597\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/joblib/parallel.py:288\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[0;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/joblib/parallel.py:288\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[0;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    121\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    684\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[1;32m    685\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 686\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    689\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[1;32m    690\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/xgboost/core.py:730\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    728\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    729\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/xgboost/sklearn.py:1519\u001B[0m, in \u001B[0;36mXGBClassifier.fit\u001B[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001B[0m\n\u001B[1;32m   1491\u001B[0m (\n\u001B[1;32m   1492\u001B[0m     model,\n\u001B[1;32m   1493\u001B[0m     metric,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1498\u001B[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001B[1;32m   1499\u001B[0m )\n\u001B[1;32m   1500\u001B[0m train_dmatrix, evals \u001B[38;5;241m=\u001B[39m _wrap_evaluation_matrices(\n\u001B[1;32m   1501\u001B[0m     missing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmissing,\n\u001B[1;32m   1502\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     feature_types\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_types,\n\u001B[1;32m   1517\u001B[0m )\n\u001B[0;32m-> 1519\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Booster \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1520\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1521\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1523\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1528\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective):\n\u001B[1;32m   1534\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m=\u001B[39m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjective\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/xgboost/core.py:730\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    728\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    729\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/xgboost/training.py:181\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mbefore_iteration(bst, i, dtrain, evals):\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 181\u001B[0m \u001B[43mbst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mafter_iteration(bst, i, dtrain, evals):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/xgboost/core.py:2051\u001B[0m, in \u001B[0;36mBooster.update\u001B[0;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[1;32m   2047\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_dmatrix_features(dtrain)\n\u001B[1;32m   2049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2050\u001B[0m     _check_call(\n\u001B[0;32m-> 2051\u001B[0m         \u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2052\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\n\u001B[1;32m   2053\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2054\u001B[0m     )\n\u001B[1;32m   2055\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2056\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dtrain, output_margin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming X_train, Y_train are defined and feature_subsets is a dictionary with your subsets\n",
    "\n",
    "best_xgb_models = {}\n",
    "\n",
    "# Define a parameter grid to search for best parameters for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    print(f\"Starting GridSearchCV for XGBoost on {feature_name}...\")\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, objective='multi:softprob', num_class=8, eval_metric='mlogloss', random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train_subset, Y_train)\n",
    "\n",
    "    best_xgb_models[feature_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {feature_name}: {grid_search.best_params_}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def train_neural_network(X_train, Y_train_encoded):\n",
    "    # Correctly defining input_shape based on X_train\n",
    "    input_shape = X_train.shape[1]  # Number of features\n",
    "\n",
    "    # Correctly setting the number of neurons in the output layer\n",
    "    Y_neurons = Y_train_encoded.shape[1]  # Number of classes\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    # Using Y_neurons for the number of output neurons dynamically\n",
    "    model.add(Dense(Y_neurons, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Moving the fit function outside of the initial return statement\n",
    "    # Assuming a modest number of epochs for demonstration; adjust as necessary\n",
    "    model.fit(X_train, Y_train_encoded, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "(3600, 8)"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_encoded.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def evaluate_model(model, X_val, Y_val):\n",
    "    # Ensure Y_val is one-hot encoded for the evaluation\n",
    "    Y_val_encoded = to_categorical(Y_val)\n",
    "    scores = model.evaluate(X_val, Y_val_encoded, verbose=0)\n",
    "    return scores[1]  # Assuming scores[1] is accuracy. scores[0] would be loss.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: chroma_cens, Accuracy: 0.3200\n",
      "Feature: chroma_cqt, Accuracy: 0.3233\n",
      "Feature: chroma_stft, Accuracy: 0.3792\n",
      "Feature: mfcc, Accuracy: 0.5275\n",
      "Feature: rmse, Accuracy: 0.2650\n",
      "Feature: spectral_bandwidth, Accuracy: 0.3108\n",
      "Feature: spectral_centroid, Accuracy: 0.3617\n",
      "Feature: spectral_contrast, Accuracy: 0.4417\n",
      "Feature: spectral_rolloff, Accuracy: 0.3558\n",
      "Feature: tonnetz, Accuracy: 0.3092\n",
      "Feature: zcr, Accuracy: 0.3158\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for feature_name, nn_model in best_nn_models.items():\n",
    "    # Extract the corresponding validation data for the feature\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "    # Evaluate the model\n",
    "    accuracy = evaluate_model(nn_model, X_val_subset, Y_val)\n",
    "    print(f\"Feature: {feature_name}, Accuracy: {accuracy:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stacked Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Meta Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Default Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "meta_model = LogisticRegression(random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LR Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# Train the meta-model\n",
    "meta_model_lr = LogisticRegression(random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Initialize the XGBoost classifier as the meta-model\n",
    "meta_model_xgb = xgb.XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    n_estimators=500,  # Increased from a default value, which might be around 100\n",
    "    learning_rate=0.05,  # Adjust learning rate to control the pace of boosting and prevent overfitting\n",
    "    max_depth=10,  # Consider setting max_depth to regulate tree complexity\n",
    "    objective='multi:softprob',\n",
    "    num_class=8,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest classifier as the meta-model\n",
    "meta_model_rf = RandomForestClassifier(\n",
    "    n_estimators=500,  # Number of trees in the forest\n",
    "    max_depth=10,  # Maximum depth of the trees to limit complexity and prevent overfitting\n",
    "    min_samples_split=2,  # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=1,  # Minimum number of samples required to be at a leaf node\n",
    "    criterion='gini',  # Function to measure the quality of a split. 'entropy' can also be used for information gain\n",
    "    random_state=42,  # Ensures a deterministic outcome for reproducible results\n",
    "    n_jobs=-1  # Use all available cores for faster training\n",
    ")\n",
    "\n",
    "# Note: Adjust the above hyperparameters based on your dataset characteristics and computational resources\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# Initialize the SVM classifier as the meta-model\n",
    "# 'probability=True' enables the predict_proba method, which is useful for getting predicted probabilities\n",
    "meta_model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NN Meta Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert target variable to categorical (one-hot encoding) for neural network\n",
    "Y_val_encoded = to_categorical(Y_val)\n",
    "\n",
    "# Define the neural network architecture with wider and more layers\n",
    "meta_model_nn = Sequential(name='Enhanced_MetaModel_NN')\n",
    "meta_model_nn.add(Dense(128, input_shape=(X_meta_train.shape[1],), activation='relu', name='Dense_Layer_1'))\n",
    "meta_model_nn.add(Dropout(0.2, name='Dropout_1'))  # Add dropout for regularization\n",
    "meta_model_nn.add(Dense(64, activation='relu', name='Dense_Layer_2'))\n",
    "meta_model_nn.add(Dropout(0.2, name='Dropout_2'))  # Add dropout for regularization\n",
    "meta_model_nn.add(Dense(64, activation='relu', name='Dense_Layer_3'))  # Additional layer\n",
    "meta_model_nn.add(Dense(Y_val_encoded.shape[1], activation='softmax', name='Output_Layer'))\n",
    "\n",
    "# Compile the meta-model\n",
    "meta_model_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF and KNN Stacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF and KNN models: 0.5758333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data for the meta-model\n",
    "# Generate predictions from RF and KNN models on the validation set\n",
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        model.predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]  # Add a new axis for stacking\n",
    "        for model in (best_rf_models[feature_name], best_knn_models[feature_name])\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF and KNN models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Prepare test data in a similar manner\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        model.predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "        for model in (best_rf_models[feature_name], best_knn_models[feature_name])\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the meta-model\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF and KNN models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF, KNN, and LR Stacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF, KNN, and LR models: 0.5941666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update the preparation of the data for the meta-model to include logistic regression predictions\n",
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF, KNN, and LR models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "# Train the meta-model with the updated training data\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Update the preparation of the test data in a similar manner to include logistic regression predictions\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the updated meta-model\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of logistic regression models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, and LR models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for chroma_cens: {'C': 10, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for chroma_cqt: {'C': 10, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for chroma_stft: {'C': 10, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['poly'],  # Kernel type\n",
    "    'degree': [2, 3, 4],  # Degree of the polynomial kernel function\n",
    "    'gamma': ['scale', 'auto'],  # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store the best SVM models for each feature subset\n",
    "best_svm_models = {}\n",
    "\n",
    "# Perform grid search to find the best parameters for the SVM model for each feature subset\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    # Initialize the SVM model\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "\n",
    "    # Set up the grid search with cross-validation\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train_subset, Y_train)\n",
    "\n",
    "    # Retrieve the best model\n",
    "    best_svm_models[feature_name] = grid_search.best_estimator_\n",
    "\n",
    "    # Optionally, print the best parameters for each feature subset\n",
    "    print(f\"Best parameters for {feature_name}: {grid_search.best_params_}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF, KNN, LR, and SVM models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[124], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Update the preparation of the data for the meta-model to include SVM predictions\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m X_meta_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhstack([\n\u001B[1;32m      3\u001B[0m     np\u001B[38;5;241m.\u001B[39mconcatenate([\n\u001B[1;32m      4\u001B[0m         best_rf_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      5\u001B[0m         best_knn_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      6\u001B[0m         best_lr_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      7\u001B[0m         best_svm_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis]  \u001B[38;5;66;03m# Add SVM predictions\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     ], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mmean(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# Average predictions from RF, KNN, LR, and now SVM models for each feature subset\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature_name \u001B[38;5;129;01min\u001B[39;00m feature_subsets\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[1;32m     10\u001B[0m ])\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;28mlen\u001B[39m(Y_val), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Reshape to have a standard 2D array\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Re-train the meta-model with the updated training data including SVM predictions\u001B[39;00m\n\u001B[1;32m     13\u001B[0m meta_model\u001B[38;5;241m.\u001B[39mfit(X_meta_train, Y_val)\n",
      "Cell \u001B[0;32mIn[124], line 7\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Update the preparation of the data for the meta-model to include SVM predictions\u001B[39;00m\n\u001B[1;32m      2\u001B[0m X_meta_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhstack([\n\u001B[1;32m      3\u001B[0m     np\u001B[38;5;241m.\u001B[39mconcatenate([\n\u001B[1;32m      4\u001B[0m         best_rf_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      5\u001B[0m         best_knn_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[1;32m      6\u001B[0m         best_lr_models[feature_name]\u001B[38;5;241m.\u001B[39mpredict_proba(val_feature_subsets[feature_name])[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis],\n\u001B[0;32m----> 7\u001B[0m         \u001B[43mbest_svm_models\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeature_name\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_feature_subsets\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeature_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[:, :, np\u001B[38;5;241m.\u001B[39mnewaxis]  \u001B[38;5;66;03m# Add SVM predictions\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     ], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mmean(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# Average predictions from RF, KNN, LR, and now SVM models for each feature subset\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature_name \u001B[38;5;129;01min\u001B[39;00m feature_subsets\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[1;32m     10\u001B[0m ])\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;28mlen\u001B[39m(Y_val), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Reshape to have a standard 2D array\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Re-train the meta-model with the updated training data including SVM predictions\u001B[39;00m\n\u001B[1;32m     13\u001B[0m meta_model\u001B[38;5;241m.\u001B[39mfit(X_meta_train, Y_val)\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/svm/_base.py:871\u001B[0m, in \u001B[0;36mBaseSVC.predict_proba\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    865\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NotFittedError(\n\u001B[1;32m    866\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredict_proba is not available when fitted with probability=False\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    867\u001B[0m     )\n\u001B[1;32m    868\u001B[0m pred_proba \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sparse_predict_proba \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sparse \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dense_predict_proba\n\u001B[1;32m    870\u001B[0m )\n\u001B[0;32m--> 871\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpred_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/sml-practical/lib/python3.10/site-packages/sklearn/svm/_base.py:911\u001B[0m, in \u001B[0;36mBaseSVC._dense_predict_proba\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    908\u001B[0m     kernel \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprecomputed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    910\u001B[0m svm_type \u001B[38;5;241m=\u001B[39m LIBSVM_IMPL\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_impl)\n\u001B[0;32m--> 911\u001B[0m pprob \u001B[38;5;241m=\u001B[39m \u001B[43mlibsvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    912\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    913\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msupport_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    914\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msupport_vectors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_n_support\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dual_coef_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_intercept_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_probA\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_probB\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[43m    \u001B[49m\u001B[43msvm_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msvm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    921\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkernel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkernel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    922\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdegree\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdegree\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    923\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef0\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoef0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pprob\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Update the preparation of the data for the meta-model to include SVM predictions\n",
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]  # Add SVM predictions\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF, KNN, LR, and now SVM models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "# Re-train the meta-model with the updated training data including SVM predictions\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Prepare the test data in a similar manner to include SVM predictions\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]  # Add SVM predictions\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the updated meta-model including SVM\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of SVM models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, LR, and SVM models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF, KNN, LR, SVM, and XGBoost models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF, KNN, LR, SVM, and XGBoost models: 0.6116666666666667\n"
     ]
    }
   ],
   "source": [
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],  # Existing SVM predictions\n",
    "        best_xgb_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]  # Add XGBoost predictions\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from all models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)  # Reshape to have a standard 2D array\n",
    "\n",
    "# Re-train the meta-model with the updated training data including XGBoost predictions\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Prepare the test data in a similar manner to include XGBoost predictions\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],  # Existing SVM predictions\n",
    "        best_xgb_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]  # Add XGBoost predictions\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the updated meta-model including XGBoost\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of XGBoost models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, LR, SVM, and XGBoost models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Previous with NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 5ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "38/38 [==============================] - 1s 7ms/step\n",
      "Test accuracy with stacked RF, KNN, LR, SVM, XGBoost, and Neural Network models: 0.6166666666666667\n"
     ]
    }
   ],
   "source": [
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_xgb_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        # Generate predictions from the NN model for the validation set and add them\n",
    "        best_nn_models[feature_name].predict(val_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)\n",
    "\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Modification for X_meta_test to include neural network predictions\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_knn_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_lr_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_xgb_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        # Add neural network predictions for the test set\n",
    "        best_nn_models[feature_name].predict(test_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from all models for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Make final predictions with the updated meta-model including all models\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of all models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, LR, SVM, XGBoost, and Neural Network models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF, XGB, SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with stacked RF, SVM, and XGBoost models: 0.6083333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC  # For SVM\n",
    "from xgboost import XGBClassifier  # For XGBoost\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Prepare X_meta_train by averaging predictions from RF, SVM, and XGB models for each feature subset\n",
    "X_meta_train = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_xgb_models[feature_name].predict_proba(val_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF, SVM, and XGB for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)\n",
    "\n",
    "# Similar preparation for X_meta_test\n",
    "X_meta_test = np.hstack([\n",
    "    np.concatenate([\n",
    "        best_rf_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_svm_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis],\n",
    "        best_xgb_models[feature_name].predict_proba(test_feature_subsets[feature_name])[:, :, np.newaxis]\n",
    "    ], axis=2).mean(axis=2)  # Average predictions from RF, SVM, and XGB for each feature subset\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Assuming meta_model is already defined and ready for training\n",
    "# Train the meta-model on the updated X_meta_train dataset\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Make final predictions with the updated meta-model\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, SVM, and XGBoost models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Meta Learners"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log Reg Meta learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meta_model_lr.fit(X_meta_train, Y_val)\n",
    "# Make final predictions with the updated meta-model including all models\n",
    "final_predictions = meta_model_lr.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy with the inclusion of all models\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with stacked RF, KNN, LR, SVM, XGBoost, and Neural Network models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Meta Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with Random Forest as meta-model: 0.5658333333333333\n"
     ]
    }
   ],
   "source": [
    "# Train the meta-model on the meta-training dataset\n",
    "meta_model_rf.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Make predictions on the meta-test dataset\n",
    "final_predictions_rf = meta_model_rf.predict(X_meta_test)\n",
    "\n",
    "# Evaluate and print the test accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy_rf = accuracy_score(Y_test, final_predictions_rf)\n",
    "print(f\"Test accuracy with Random Forest as meta-model: {test_accuracy_rf}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost Meta Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with Random Forest as meta-model: 0.5658333333333333\n"
     ]
    }
   ],
   "source": [
    "# Train the meta-model on the meta-training dataset\n",
    "meta_model_xgb.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Make predictions on the meta-test dataset\n",
    "final_predictions_xgb = meta_model_xgb.predict(X_meta_test)\n",
    "\n",
    "# Evaluate and print the test accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy_xgb = accuracy_score(Y_test, final_predictions_rf)\n",
    "print(f\"Test accuracy with Random Forest as meta-model: {test_accuracy_rf}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NN as Meta Learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4736 - accuracy: 0.8396 - val_loss: 2.0629 - val_accuracy: 0.5125\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4530 - accuracy: 0.8469 - val_loss: 2.0073 - val_accuracy: 0.5125\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3802 - accuracy: 0.8552 - val_loss: 2.0673 - val_accuracy: 0.5667\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3721 - accuracy: 0.8646 - val_loss: 2.0467 - val_accuracy: 0.5667\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3069 - accuracy: 0.8927 - val_loss: 1.9803 - val_accuracy: 0.5542\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2978 - accuracy: 0.8958 - val_loss: 2.0336 - val_accuracy: 0.5458\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2934 - accuracy: 0.8896 - val_loss: 2.1517 - val_accuracy: 0.5333\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2976 - accuracy: 0.8833 - val_loss: 2.1511 - val_accuracy: 0.5292\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2756 - accuracy: 0.8990 - val_loss: 2.1267 - val_accuracy: 0.5542\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2523 - accuracy: 0.9062 - val_loss: 2.1667 - val_accuracy: 0.5417\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2273 - accuracy: 0.9323 - val_loss: 2.2142 - val_accuracy: 0.5500\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2205 - accuracy: 0.9219 - val_loss: 2.3175 - val_accuracy: 0.5292\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2230 - accuracy: 0.9187 - val_loss: 2.1395 - val_accuracy: 0.5542\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2252 - accuracy: 0.9240 - val_loss: 2.3148 - val_accuracy: 0.5458\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2129 - accuracy: 0.9271 - val_loss: 2.2560 - val_accuracy: 0.5458\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the meta-model with early stopping\n",
    "history = meta_model_nn.fit(X_meta_train, Y_val_encoded, epochs=100, batch_size=32,\n",
    "                            validation_split=0.2, verbose=1, callbacks=[early_stopping])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 2.3029 - accuracy: 0.5342\n",
      "Test Loss: 2.302931785583496\n",
      "Test Accuracy: 0.534166693687439\n"
     ]
    }
   ],
   "source": [
    "Y_test_encoded = to_categorical(Y_test)\n",
    "test_loss, test_accuracy = meta_model_nn.evaluate(X_meta_test, Y_test_encoded, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Stacked Bootstrap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with bootstrapped and stacked RF, KNN, and LR models: 0.5933333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Define the number of bootstrap samples and models\n",
    "n_bootstrap_samples = 10\n",
    "\n",
    "# Placeholder for trained bootstrap models\n",
    "bootstrap_models_rf = {feature_name: [] for feature_name in feature_subsets.keys()}\n",
    "bootstrap_models_knn = {feature_name: [] for feature_name in feature_subsets.keys()}\n",
    "bootstrap_models_lr = {feature_name: [] for feature_name in feature_subsets.keys()}\n",
    "\n",
    "# Train bootstrap models for RandomForest\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    for _ in range(n_bootstrap_samples):\n",
    "        # Create a bootstrap sample\n",
    "        X_boot, Y_boot = resample(X_train_subset, Y_train)\n",
    "        # Initialize and train the model on the bootstrap sample\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_boot, Y_boot)\n",
    "        bootstrap_models_rf[feature_name].append(model)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Train bootstrap models for KNN\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    for _ in range(n_bootstrap_samples):\n",
    "        X_boot, Y_boot = resample(X_train_subset, Y_train)\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        knn_model.fit(X_boot, Y_boot)\n",
    "        bootstrap_models_knn[feature_name].append(knn_model)\n",
    "\n",
    "# Train bootstrap models for Logistic Regression\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    for _ in range(n_bootstrap_samples):\n",
    "        X_boot, Y_boot = resample(X_train_subset, Y_train)\n",
    "        lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        lr_model.fit(X_boot, Y_boot)\n",
    "        bootstrap_models_lr[feature_name].append(lr_model)\n",
    "\n",
    "# Generate averaged predictions for the validation set\n",
    "# This assumes `val_feature_subsets` is already prepared similarly to `feature_subsets`\n",
    "X_meta_train = np.hstack([\n",
    "    np.mean([\n",
    "        np.mean([model.predict_proba(val_feature_subsets[feature_name]) for model in models], axis=0)\n",
    "        for models in (bootstrap_models_rf[feature_name], bootstrap_models_knn[feature_name], bootstrap_models_lr[feature_name])\n",
    "    ], axis=0)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_val), -1)\n",
    "\n",
    "# Train the meta-model\n",
    "meta_model.fit(X_meta_train, Y_val)\n",
    "\n",
    "# Assuming `test_feature_subsets` is prepared, generate predictions for the test set\n",
    "X_meta_test = np.hstack([\n",
    "    np.mean([\n",
    "        np.mean([model.predict_proba(test_feature_subsets[feature_name]) for model in models], axis=0)\n",
    "        for models in (bootstrap_models_rf[feature_name], bootstrap_models_knn[feature_name], bootstrap_models_lr[feature_name])\n",
    "    ], axis=0)\n",
    "    for feature_name in feature_subsets.keys()\n",
    "]).reshape(len(Y_test), -1)\n",
    "\n",
    "# Final predictions with the meta-model\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with bootstrapped and stacked RF, KNN, and LR models: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Best Model per Feature Subset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: chroma_cens, Best Model: SVM, Accuracy: 0.33\n",
      "Feature: chroma_cqt, Best Model: SVM, Accuracy: 0.35\n",
      "Feature: chroma_stft, Best Model: RF, Accuracy: 0.39\n",
      "Feature: mfcc, Best Model: SVM, Accuracy: 0.56\n",
      "Feature: rmse, Best Model: RF, Accuracy: 0.27\n",
      "Feature: spectral_bandwidth, Best Model: RF, Accuracy: 0.33\n",
      "Feature: spectral_centroid, Best Model: SVM, Accuracy: 0.38\n",
      "Feature: spectral_contrast, Best Model: XGB, Accuracy: 0.48\n",
      "Feature: spectral_rolloff, Best Model: SVM, Accuracy: 0.37\n",
      "Feature: tonnetz, Best Model: SVM, Accuracy: 0.32\n",
      "Feature: zcr, Best Model: SVM, Accuracy: 0.34\n",
      "Test accuracy with the best model from each feature subset ensemble: 0.565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC  # For SVM with RBF kernel\n",
    "from xgboost import XGBClassifier  # For XGBoost\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `feature_subsets`, `X_train`, `Y_train`, `X_val`, `Y_val`, `X_test`, `Y_test`, `val_feature_subsets`, and `test_feature_subsets` are already defined\n",
    "\n",
    "best_models = {}\n",
    "model_accuracies = {}  # To store the best accuracy for each feature subset\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "\n",
    "    models = {\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'RF': RandomForestClassifier(random_state=42),\n",
    "        'LR': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'SVM': SVC(probability=True, kernel='rbf', random_state=42),  # SVM with RBF kernel\n",
    "        'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)  # XGBoost\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train_subset, Y_train)\n",
    "        accuracy = model.score(X_val_subset, Y_val)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = (model_name, model)\n",
    "\n",
    "    best_models[feature_name] = best_model\n",
    "    model_accuracies[feature_name] = best_accuracy  # Store the best accuracy\n",
    "\n",
    "# Print out the best model and its accuracy for each feature subset\n",
    "for feature_name, (model_name, _) in best_models.items():\n",
    "    accuracy = model_accuracies[feature_name]\n",
    "    print(f\"Feature: {feature_name}, Best Model: {model_name}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Ensemble selected models for final prediction\n",
    "ensemble_predictions = np.zeros((len(Y_test), len(np.unique(Y_train))))\n",
    "\n",
    "for feature_name, (model_type, model) in best_models.items():\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        predictions = model.predict_proba(X_test_subset)\n",
    "    else:  # For models like SVM that might not support predict_proba by default\n",
    "        predictions = model.decision_function(X_test_subset)\n",
    "        # Normalize SVM decision function output to [0, 1] for ensemble\n",
    "        predictions = (predictions - predictions.min()) / (predictions.max() - predictions.min())\n",
    "    ensemble_predictions += predictions\n",
    "\n",
    "# Final ensemble prediction\n",
    "final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with the best model from each feature subset ensemble: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## With Weighting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with weighted ensemble: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty dictionary to store validation accuracies\n",
    "validation_accuracies = {}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for feature_name, X_train_subset in feature_subsets.items():\n",
    "    # Track the best model for this subset\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    X_val_subset = val_feature_subsets[feature_name]\n",
    "\n",
    "    # Define your models here, including SVM and XGBoost\n",
    "    models = {\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'RF': RandomForestClassifier(random_state=42),\n",
    "        'LR': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'SVM': SVC(probability=True, kernel='rbf', random_state=42),  # SVM with RBF kernel\n",
    "        'XGB': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)  # XGBoost\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train_subset, Y_train)\n",
    "        accuracy = model.score(X_val_subset, Y_val)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = (model_name, model)\n",
    "\n",
    "    # Store the best model and its validation accuracy\n",
    "    best_models[feature_name] = best_model\n",
    "    validation_accuracies[feature_name] = best_accuracy\n",
    "\n",
    "# Ensemble selected models for final prediction with weighted predictions\n",
    "ensemble_predictions = np.zeros((len(Y_test), len(np.unique(Y_train))))\n",
    "\n",
    "for feature_name, (model_type, model) in best_models.items():\n",
    "    X_test_subset = test_feature_subsets[feature_name]\n",
    "    weight = validation_accuracies[feature_name]\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        predictions = model.predict_proba(X_test_subset) * weight\n",
    "    else:  # For models like SVM that might not support predict_proba by default\n",
    "        # Normalize decision_function output to [0,1] and apply weight\n",
    "        decision_values = model.decision_function(X_test_subset)\n",
    "        predictions = (decision_values - decision_values.min()) / (decision_values.max() - decision_values.min())\n",
    "        predictions *= weight\n",
    "\n",
    "    # Sum the weighted predictions\n",
    "    ensemble_predictions += predictions\n",
    "\n",
    "# Normalize the ensemble predictions to ensure they form a valid probability distribution\n",
    "ensemble_predictions /= ensemble_predictions.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Make final predictions based on the weighted ensemble\n",
    "final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, final_predictions)\n",
    "print(f\"Test accuracy with weighted ensemble: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
